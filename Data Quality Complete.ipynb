{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run everything in the **Setup** section. \n",
    "    - Make sure to change the working directory to **your** working directory. The code for this is already there.\n",
    "    - Make sure the Excel document for logging the scores also exists in your working directory, and that the file name is correct.\n",
    "\n",
    "2. Determine *if the test needs to be run* by having a good understanding of what each test is doing. \n",
    "    - Please refer to this document [here](https://086gc.sharepoint.com/:x:/r/sites/PacificSalmonTeam/Shared%20Documents/General/02%20-%20PSSI%20Secretariat%20Teams/04%20-%20Strategic%20Salmon%20Data%20Policy%20and%20Analytics/02%20-%20Data%20Governance/00%20-%20Projects/10%20-%20Data%20Quality/Presentation/DQP%20Demo.xlsx?d=wc15abe6743954df980a05f09fe99a560&csf=1&web=1&e=CJeb6h)\n",
    "\n",
    "3. Some requirements for the datasets:\n",
    "    - The data must be on the **first sheet** in the Excel document.\n",
    "    - The **first row** must be the column names. \n",
    "    - The test won't run if the Excel file is open\n",
    "\n",
    "4. After running all the tests, the Excel document for logging the scores can be uploaded to Sharepoint using the function \"Saving the file to sharepoint\". \n",
    "\n",
    "Note: The Output Reports are used for when a data steward is asking about why their dataset gets a certain score. If the metric is not in Output Reports, then running the test itself will generate an output that can be put into a report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run everything in the set up, and double check the working directory so that the data can be read from that same directory.\n",
    "\n",
    "All of these functions are used in the process of calculating data quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onakd\\AppData\\Local\\Temp\\1\\ipykernel_27876\\1945100915.py:4: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Clear memory\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import nbformat\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support functions to allow running cells from other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selected_cells(notebook_path, cell_indices):       \n",
    "    # Load the notebook       \n",
    "    with open(notebook_path) as f:           \n",
    "        nb = nbformat.read(f, as_version=4)   \n",
    "\n",
    "    # Get the cells to run       \n",
    "    selected_cells = [nb.cells[i] for i in cell_indices]   \n",
    "\n",
    "    # Execute the selected cells       \n",
    "    for cell in selected_cells:           \n",
    "        if cell.cell_type == 'code':               \n",
    "            exec(cell.source, globals())\n",
    "\n",
    "def run_selected_cells_from_util(util_folder, notebook_name, cell_indices):       \n",
    "    notebook_path = os.path.join(util_folder, notebook_name)       \n",
    "    run_selected_cells(notebook_path, cell_indices)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set to the correct working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to the same place where you saved the test datasets\n",
    "# os.chdir('C:/Users/luos/OneDrive - DFO-MPO/Python') #change directory\n",
    "os.getcwd()  # check where the directory is (and whether the change was successful or not)\n",
    "GLOBAL_USER = \"OnakD\"\n",
    "GLOBAL_DATASET = \"SEP Production Plans\"\n",
    "GLOBAL_DATAFILE = \"2015-sep-production-plan-en (1).csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read either csv or xlsx data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 0: Reading the dataset file\n",
    "def read_data(dataset_path):\n",
    "    _, file_extension = os.path.splitext(dataset_path)\n",
    "    if file_extension == \".csv\":\n",
    "        try:  \n",
    "            df = pd.read_csv(dataset_path, encoding=\"utf-8-sig\")  \n",
    "        except UnicodeDecodeError:  \n",
    "            df = pd.read_csv(dataset_path, encoding=\"cp1252\") \n",
    "    elif file_extension == \".xlsx\":\n",
    "        df = pd.read_excel(dataset_path)\n",
    "    else:\n",
    "        print(\"Unsupported file type\")\n",
    "        df = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to log the scores into an xlsx file (already created, existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log a new row into the DQS_Log_XX.xlsx file\n",
    "def log_score(test_name, dataset_name, score, selected_columns, threshold=None):\n",
    "    # Convert score to a percentage\n",
    "    percentage_score = score\n",
    "\n",
    "    # Load the Excel file into a DataFrame\n",
    "    log_file = \"DQS_Log_Beta.xlsx\"\n",
    "\n",
    "    # Set threshold to \"No threshold\" if it is not provided\n",
    "    if threshold is None:\n",
    "        threshold_value = \"no threshold\"\n",
    "    else:\n",
    "        threshold_value = threshold\n",
    "\n",
    "    # If selected_columns is None, assume \"All\" was tested\n",
    "    if selected_columns is None:\n",
    "        columns_tested = \"All columns\"\n",
    "    else:\n",
    "        # Convert selected_columns list to a string if specific columns are provided\n",
    "        columns_tested = \", \".join(selected_columns)\n",
    "\n",
    "    # Try loading the existing Excel file\n",
    "    try:\n",
    "        df = read_data(log_file)\n",
    "    except FileNotFoundError:\n",
    "        # Create an empty DataFrame if file doesn't exist (shouldn't be the case if you already created it)\n",
    "        df = pd.DataFrame(\n",
    "            columns=[\"Dataset\", \"Test\", \"Threshold\", \"Date_Calculated\", \"Score\"]\n",
    "        )\n",
    "\n",
    "    # Prepare the new row as a DataFrame\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Dataset\": [dataset_name],\n",
    "            \"Columns_Tested\": [columns_tested],  # Add the list of columns tested\n",
    "            \"Test\": [test_name],\n",
    "            \"Date_Calculated\": [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"Threshold\": [threshold_value],\n",
    "            \"Score\": [percentage_score],\n",
    "            \"User\": GLOBAL_USER\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(log_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract dataset name from a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(dataset_path):\n",
    "    # Extract the file name from the path (e.g., 'Dataset_A.csv')\n",
    "    file_name = os.path.basename(dataset_path)\n",
    "    # Split the file name to remove the extension (e.g., 'Dataset_A')\n",
    "    dataset_name = os.path.splitext(file_name)[0]\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 1 (C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is best run on CSV data where the column names are in the first row. It can also accept files that are in xlsx formats but it will only take data from the first sheet if there are more than one sheet in the excel file.\n",
    "\n",
    "Limitations: It will not check for differences in capitalization of the same word (since all the words will be changed to lower case before the similarity score is calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for C1 \n",
    "run_selected_cells_from_util('utils', 'consistancy_utils.ipynb', [2])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "                     Regional Area Program Code       Project  \\\n",
      "0                      BC Interior          CDP     Deadman R   \n",
      "1                      BC Interior          CDP     Deadman R   \n",
      "2                      BC Interior          CDP     Deadman R   \n",
      "3                      BC Interior          CDP     Deadman R   \n",
      "4                      BC Interior          OPS   Gates Sp Ch   \n",
      "..                             ...          ...           ...   \n",
      "624             Lower Fraser River          OPS  Weaver Sp Ch   \n",
      "625  Yukon and Transboundary River          OPS    Snettisham   \n",
      "626  Yukon and Transboundary River          OPS    Snettisham   \n",
      "627  Yukon and Transboundary River          OPS    Snettisham   \n",
      "628  Yukon and Transboundary River          OPS    Snettisham   \n",
      "\n",
      "                                   Project Contact  Species     Run  \\\n",
      "0    CA - Central Interior, Boston Bar to 100 Mile     Coho    Fall   \n",
      "1    CA - Central Interior, Boston Bar to 100 Mile     Coho    Fall   \n",
      "2    CA - Central Interior, Boston Bar to 100 Mile     Coho    Fall   \n",
      "3    CA - Central Interior, Boston Bar to 100 Mile     Coho    Fall   \n",
      "4                  WEM - Tenderfoot Creek Hatchery  Sockeye  Summer   \n",
      "..                                             ...      ...     ...   \n",
      "624                  WEM - Chehalis River Hatchery  Sockeye    Fall   \n",
      "625              SEP Yukon-Transboundary Biologist  Sockeye  Summer   \n",
      "626              SEP Yukon-Transboundary Biologist  Sockeye  Summer   \n",
      "627              SEP Yukon-Transboundary Biologist  Sockeye  Summer   \n",
      "628              SEP Yukon-Transboundary Biologist  Sockeye  Summer   \n",
      "\n",
      "              Stock Stock CU Index                            Stock CU Name  \\\n",
      "0         Deadman R           CO-7                           LOWER THOMPSON   \n",
      "1         Deadman R           CO-7                           LOWER THOMPSON   \n",
      "2         Deadman R           CO-7                           LOWER THOMPSON   \n",
      "3         Deadman R           CO-7                           LOWER THOMPSON   \n",
      "4          Gates Cr      SEL-06-01       ANDERSON/SETON-EARLY SUMMER TIMING   \n",
      "..              ...            ...                                      ...   \n",
      "624    Weaver Sp Ch      SEL-03-04  HARRISON-UPSTREAM MIGRATING-LATE TIMING   \n",
      "625  King Salmon Lk            NaN                                      NaN   \n",
      "626      Tahltan Lk            NaN                                      NaN   \n",
      "627   Tatsamenie Lk            NaN                                      NaN   \n",
      "628   Tatsamenie Lk            NaN                                      NaN   \n",
      "\n",
      "    Stock Stat Area  ...  Egg Target  Production Activity Type  \\\n",
      "0               29F  ...         -                 Transfer In   \n",
      "1               29F  ...         -                 Transfer In   \n",
      "2               29F  ...         -                     Release   \n",
      "3               29F  ...         -                     Release   \n",
      "4               29F  ...   27,000,000                  Release   \n",
      "..              ...  ...          ...                      ...   \n",
      "624             29D  ...   50,000,000                  Release   \n",
      "625             130  ...      250,000                  Release   \n",
      "626             120  ...    6,000,000                  Release   \n",
      "627             130  ...    2,000,000                  Release   \n",
      "628             130  ...         -                     Release   \n",
      "\n",
      "    Release/ Transfer Stage Release Site/ Transfer Project Release Stat Area  \\\n",
      "0                     Smolt                       Spius Cr               NaN   \n",
      "1                     Smolt                   Thompson R N               NaN   \n",
      "2                  Smolt 1+                      Deadman R               29F   \n",
      "3                  Smolt 1+                      Deadman R               29F   \n",
      "4                  Chan Fry                       Gates Cr               29F   \n",
      "..                      ...                            ...               ...   \n",
      "624                Chan Fry                   Weaver Sp Ch               29D   \n",
      "625                   Unfed                 King Salmon Lk               130   \n",
      "626                 Fed Fry                     Tahltan Lk               120   \n",
      "627                   Unfed                  Tatsamenie Lk               130   \n",
      "628                 Fed Fry                  Tatsamenie Lk               130   \n",
      "\n",
      "     Release/ Transfer Target  Mark Type Clip Type  Target # to Mark   \\\n",
      "0                       15,000       NaN       NaN                NaN   \n",
      "1                       15,000       NaN       NaN                NaN   \n",
      "2                       15,000       NaN       NaN                NaN   \n",
      "3                       15,000       NaN       NaN                NaN   \n",
      "4                   13,500,000       NaN       NaN                NaN   \n",
      "..                         ...       ...       ...                ...   \n",
      "624                 46,800,000       NaN       NaN                NaN   \n",
      "625                    225,000   Thermal       NaN            225,000   \n",
      "626                  5,000,000   Thermal       NaN          5,000,000   \n",
      "627                  1,775,000       NaN       NaN                NaN   \n",
      "628                    225,000   Thermal       NaN          2,000,000   \n",
      "\n",
      "                                               Comment  \n",
      "0    Smolts will be transferred from Spius Creek ha...  \n",
      "1    2015: alternate interim rearing location added...  \n",
      "2    Release of 7,000- 15,000 coho smolts (pending ...  \n",
      "3    2015: alternate interim rearing location added...  \n",
      "4    Production target is based on adult channel lo...  \n",
      "..                                                 ...  \n",
      "624                                                NaN  \n",
      "625  Production targets set by the Enhancement subc...  \n",
      "626  Eggs are transported to Snettisham hatchery in...  \n",
      "627  Egg target is ~30% of available brood, up to 2...  \n",
      "628  This group is set aside for extended rearing o...  \n",
      "\n",
      "[629 rows x 22 columns]\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.9847642584004557\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{GLOBAL_DATASET}/{GLOBAL_DATAFILE}\"\n",
    "    # Test Consistency Calculations\n",
    "\n",
    "    processed_df = process_and_calculate_similarity(\n",
    "        dataset_path=datafilepath,\n",
    "        column_names=[\"Regional Area\", \"Project\", \"Project Contact\", \"Stock\", \"Stock CU Name\", \"Release Site/ Transfer Project\", \"Mark Type\", \"Comment\"],\n",
    "        threshold=0.91\n",
    "    )\n",
    "\n",
    "    # processed_df['Overall Consistency Score'].min()\n",
    "    print(processed_df)\n",
    "except MemoryError as e:\n",
    "    print(\"Dataset is too large for this test, out of memory!\")\n",
    "    print(e)\n",
    "except KeyError as e:\n",
    "    print(\"Issue with column names, are you sure you entered them correctly?\")\n",
    "    print(e)\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Did not find dataset, make sure you have provided the correct name.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 2 (C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of datasets with a reference list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compared columns in question must be identical to the ref list, otherwise they will be penalized more harshly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for C2\n",
    "run_selected_cells_from_util('utils', 'consistancy_utils.ipynb', [4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_mapping = {\n",
    "#     \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "#     \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "# }  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "# process_and_calculate_similarity_ref(\n",
    "#     dataset_path=\"data/test/2024-03-28 1_qryThermal_NatEmerg.xlsx\",\n",
    "#     column_mapping=column_mapping,\n",
    "#     ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\",\n",
    "#     threshold=1,\n",
    "#     Stop_Words=[\"\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 1 (A1, Mixed Data Types, Symbols in Numerics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether there are symbols in numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for A1\n",
    "run_selected_cells_from_util('utils', 'accuracy_utils.ipynb', [2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{GLOBAL_DATASET}/{GLOBAL_DATAFILE}\"\n",
    "    score = accuracy_score(\n",
    "        dataset_path=datafilepath,\n",
    "        selected_columns=[\"Egg Target \", \"Release/ Transfer Target\", \"Target # to Mark\"],\n",
    "    )\n",
    "    print(score)\n",
    "except KeyError as e:\n",
    "    print(\"Issue with column names, are you sure you entered them correctly?\")\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(\"Test failed!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 2 (A2 Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for A2\n",
    "run_selected_cells_from_util('utils', 'accuracy_utils.ipynb', [4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with column names, are you sure you entered them correctly?\n",
      "'Egg Target '\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{GLOBAL_DATASET}/{GLOBAL_DATAFILE}\"\n",
    "    outliers = find_outliers_iqr(\n",
    "        dataset_path=datafilepath,\n",
    "        selected_columns = [\"Egg Target \", \"Release/ Transfer Target\", \"Target # to Mark\"],\n",
    "        threshold=0.90,\n",
    "        minimum_score=0.85,\n",
    "    )\n",
    "    print(outliers)\n",
    "except KeyError as e:\n",
    "    print(\"Issue with column names, are you sure you entered them correctly?\")\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(\"Test failed!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 3 (A3 Duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for A3\n",
    "run_selected_cells_from_util('utils', 'accuracy_utils.ipynb', [6]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [ï»¿Regional Area, Program Code, Project, Project Contact, Species, Run, Stock, Stock CU Index, Stock CU Name, Stock Stat Area, Objective, Production Strategy Level,  Egg Target , Production Activity Type, Release/ Transfer Stage, Release Site/ Transfer Project, Release Stat Area,  Release/ Transfer Target , Mark Type, Clip Type,  Target # to Mark , Comment]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 22 columns]\n",
      "\n",
      "Duplication Score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{GLOBAL_DATASET}/{GLOBAL_DATAFILE}\"\n",
    "    find_duplicates_and_percentage(\n",
    "        dataset_path=datafilepath\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Test failed!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is for removing a column that meets the threshold of the percentage of blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_test(dataset_path, exclude_columns=[], threshold=0.75):\n",
    "    dataset = read_data(dataset_path)\n",
    "\n",
    "    # Exclude the 'Comment' column if it exists in the dataset\n",
    "    if \"Comment\" in dataset.columns:\n",
    "        dataset = dataset.drop(columns=[\"Comment\"])\n",
    "\n",
    "    # Exclude columns in exclude_columns if they exist in the dataset\n",
    "    dataset = dataset.drop(\n",
    "        columns=[col for col in exclude_columns if col in dataset.columns]\n",
    "    )\n",
    "\n",
    "    # Calculate the percentage of non-null (non-missing) values in each column\n",
    "    is_null_percentage = dataset.isna().mean()\n",
    "\n",
    "    # Identify columns with non-null percentage less than or equal to the threshold\n",
    "    columns_to_keep = is_null_percentage[is_null_percentage <= threshold].index\n",
    "\n",
    "    # Keep columns that exceed the threshold of non-null values\n",
    "    dataset2 = dataset[columns_to_keep]\n",
    "\n",
    "    # Calculate the actual percentage of non-missing values in the dataset\n",
    "    total_non_missing = dataset2.notna().sum().sum()\n",
    "    total_obs = dataset2.shape[0] * dataset2.shape[1]\n",
    "    completeness_score = total_non_missing / total_obs\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Completeness (P)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=None,\n",
    "        threshold=threshold,\n",
    "        score=completeness_score,\n",
    "    )\n",
    "\n",
    "    return completeness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"North and Central Coast NuSEDS_20241004.xlsx\"\n",
    "# \"West Coast Vancouver Island NuSEDS_20241004.xlsx\"\n",
    "# \"Yukon and Transboundary NuSEDS_20241004.xlsx\"\n",
    "try:\n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{GLOBAL_DATASET}/{GLOBAL_DATAFILE}\"\n",
    "    completeness_test(\n",
    "        datafilepath,\n",
    "        threshold=0.75,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Test failed!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def calc_timeliness(refresh_date, cycle_day):\n",
    "    refresh_date = pd.to_datetime(refresh_date)\n",
    "    unupdate_cycle = np.max([((datetime.now() - refresh_date).days / cycle_day) - 1, 0])\n",
    "\n",
    "    # unupdate_cycle = np.floor((datetime.now() - refresh_date).days/cycle_day)\n",
    "    # print((datetime.now() - refresh_date).days/cycle_day)\n",
    "    return np.max([0, 100 - (unupdate_cycle * (100 / 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.30136986301369"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_timeliness(\"2022-12-01\", cycle_day=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Reports\n",
    "Run all the functions above first before running this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that output reports can be generated through the data quality tests of\n",
    "<p>    - Consistency type 1\n",
    "<p>    - Accuracy type 2\n",
    "<p>    - Accuracy type 3\n",
    "<p>    - Completeness\n",
    "<p>          \n",
    "<p>  *Completeness test does not require an output report (just find the blanks in the dataset). The rest can be found below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_datasets(dataset_path, column_mapping, ref_dataset_path=None):\n",
    "    # Read the data file\n",
    "    df = read_data(dataset_path)\n",
    "\n",
    "    # Initialize ref_df if a ref dataset is provided\n",
    "    if ref_dataset_path:\n",
    "        df_ref = read_data(ref_dataset_path)\n",
    "        ref_data = True  # Flag to indicate we are using a ref dataset\n",
    "    else:\n",
    "        ref_data = False  # No ref dataset, compare within the same dataset\n",
    "\n",
    "    for selected_column, m_selected_column in column_mapping.items():\n",
    "        if ref_data:\n",
    "            # Compare to ref dataset\n",
    "            unique_observations = get_names_used_for_column(df_ref, m_selected_column)\n",
    "        else:\n",
    "            # Use own column for comparison\n",
    "            unique_observations = get_names_used_for_column(df, selected_column)\n",
    "\n",
    "        # Iterate over each row in the selected column\n",
    "        column_results = []\n",
    "        for value in df[selected_column]:\n",
    "            # Check if the value exists in unique_observations and append the result to column_results\n",
    "            if pd.isnull(value):\n",
    "                column_results.append(\n",
    "                    False\n",
    "                )  # or True, depending on how you want to handle NaN values\n",
    "            else:\n",
    "                column_results.append(value in unique_observations)\n",
    "\n",
    "        # Add the results as a new column in the DataFrame\n",
    "        df[selected_column + \"_comparison\"] = column_results\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/test/Salmonid_Enhancement_Program_Releases.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m column_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOCK_CU_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCU_Display\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOCK_CU_INDEX\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFULL_CU_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m }  \u001b[38;5;66;03m# the pattern for comparison is 'dataset column' : 'reference column'\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcompare_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/test/Salmonid_Enhancement_Program_Releases.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_dataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m, in \u001b[0;36mcompare_datasets\u001b[1;34m(dataset_path, column_mapping, ref_dataset_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare_datasets\u001b[39m(dataset_path, column_mapping, ref_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Read the data file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Initialize ref_df if a ref dataset is provided\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ref_dataset_path:\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m(dataset_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m      6\u001b[0m         dataset_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp1252\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m     )  \u001b[38;5;66;03m# sometimes if the function has issue reading a csv file, include: , encoding = 'cp1252')\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_extension \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported file type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:1376\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:1250\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1248\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1253\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1254\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/test/Salmonid_Enhancement_Program_Releases.xlsx'"
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "    \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "}  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "compare_datasets(\n",
    "    dataset_path=\"data/test/Salmonid_Enhancement_Program_Releases.xlsx\",\n",
    "    column_mapping=column_mapping,\n",
    "    ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Using isdigit to find non-numerical entries\n",
    "def find_non_digits(s):\n",
    "    # Ensure the value is treated as a string\n",
    "    s = str(s)\n",
    "    return [char for char in s if not (char.isdigit() or char == \".\")]\n",
    "\n",
    "\n",
    "# Function 2 : Check if each row has only numbers in each selected column and add results as new columns\n",
    "def add_only_numbers_columns(dataset_path, selected_columns):\n",
    "    adf = read_data(dataset_path)\n",
    "    selected_columns = [col for col in adf.columns if col in selected_columns]\n",
    "\n",
    "    for column_name in selected_columns:\n",
    "        adf[column_name + \"_Only_Numbers\"] = adf[column_name].apply(\n",
    "            lambda x: len(find_non_digits(x)) == 0\n",
    "        )\n",
    "\n",
    "    return adf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_only_numbers_columns(\n",
    "    dataset_path=\"data/test/SEP Facilities.xlsx\", selected_columns=[\"LicNo\", \"FRN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-11-28 16:53:40'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(dataset_path):\n",
    "    # Extract the file name from the path (e.g., 'Dataset_A.csv')\n",
    "    file_name = os.path.basename(dataset_path)\n",
    "    # Split the file name to remove the extension (e.g., 'Dataset_A')\n",
    "    dataset_name = os.path.splitext(file_name)[0]\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More up to date code for the score log can be found in the \"Setup\" section, the code here is treated more as a testing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log a new row into the DQS_Log.xlsx file\n",
    "def log_score(test_name, dataset_name, score, threshold=None):\n",
    "    # Convert score to a percentage\n",
    "    percentage_score = score * 100\n",
    "\n",
    "    # Load the Excel file into a DataFrame\n",
    "    log_file = \"DQS_Log_Test.xlsx\"\n",
    "\n",
    "    # Set threshold to \"No threshold\" if it is not provided\n",
    "    if threshold is None:\n",
    "        threshold_value = \"no threshold\"\n",
    "    else:\n",
    "        threshold_value = threshold\n",
    "    # Try loading the existing Excel file\n",
    "    try:\n",
    "        df = read_data(log_file)\n",
    "    except FileNotFoundError:\n",
    "        # Create an empty DataFrame if file doesn't exist (shouldn't be the case if you already created it)\n",
    "        df = pd.DataFrame(\n",
    "            columns=[\"Dataset\", \"Test\", \"Threshold\", \"Date_Calculated\", \"Score\"]\n",
    "        )\n",
    "\n",
    "    # Prepare the new row as a DataFrame\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Dataset\": [dataset_name],\n",
    "            \"Test\": [test_name],\n",
    "            \"Date_Calculated\": [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"Threshold\": [threshold_value],\n",
    "            \"Score\": [percentage_score],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(log_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the file to sharepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to copy the log file to another folder\n",
    "def copy_log_file(destination_folder):\n",
    "    # Define the name of the file and the current working directory\n",
    "    log_file = \"DQS_Log_Test.xlsx\"\n",
    "\n",
    "    # Get the current working directory (if needed)\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Define the source path (current working directory + file)\n",
    "    source_path = os.path.join(current_directory, log_file)\n",
    "\n",
    "    # Define the destination path (destination folder + file)\n",
    "    destination_path = os.path.join(destination_folder, log_file)\n",
    "\n",
    "    # Copy the file to the destination folder\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print(f\"File copied to {destination_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function when saving the excel document from the working directory to Sharepoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the destination folder where you want to copy the file\n",
    "destination_folder = \"C:/Users/EwertM/Documents/Portal/DataQuality\"\n",
    "\n",
    "copy_log_file(destination_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
