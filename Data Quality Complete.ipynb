{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run everything in the **Setup** section. \n",
    "    - Make sure to change the working directory to **your** working directory. The code for this is already there.\n",
    "    - Make sure the Excel document for logging the scores also exists in your working directory, and that the file name is correct.\n",
    "\n",
    "2. Determine *if the test needs to be run* by having a good understanding of what each test is doing. \n",
    "    - Please refer to this document [here](https://086gc.sharepoint.com/:x:/r/sites/PacificSalmonTeam/Shared%20Documents/General/02%20-%20PSSI%20Secretariat%20Teams/04%20-%20Strategic%20Salmon%20Data%20Policy%20and%20Analytics/02%20-%20Data%20Governance/00%20-%20Projects/10%20-%20Data%20Quality/Presentation/DQP%20Demo.xlsx?d=wc15abe6743954df980a05f09fe99a560&csf=1&web=1&e=CJeb6h)\n",
    "\n",
    "3. Some requirements for the datasets:\n",
    "    - The data must be on the **first sheet** in the Excel document.\n",
    "    - The **first row** must be the column names. \n",
    "    - The test won't run if the Excel file is open\n",
    "\n",
    "4. After running all the tests, the Excel document for logging the scores can be uploaded to Sharepoint using the function \"Saving the file to sharepoint\". \n",
    "\n",
    "Note: The Output Reports are used for when a data steward is asking about why their dataset gets a certain score. If the metric is not in Output Reports, then running the test itself will generate an output that can be put into a report.  \n",
    "\n",
    "See the [Readme](/README.md) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run everything in the set up, and double check the working directory so that the data can be read from that same directory.\n",
    "\n",
    "All of these functions are used in the process of calculating data quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onakd\\AppData\\Local\\Temp\\1\\ipykernel_56356\\1945100915.py:4: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Clear memory\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import nbformat\n",
    "import gc\n",
    "\n",
    "# Import dimensions\n",
    "from dimensions.consistency import Consistency\n",
    "from dimensions.accuracy import Accuracy\n",
    "from dimensions.completeness import Completeness\n",
    "from dimensions.uniqueness import Uniqueness\n",
    "from dimensions.utils import calculate_dimension_score, calculate_DQ_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set to the correct working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to the same place where you saved the test datasets\n",
    "# os.chdir('C:/Users/luos/OneDrive - DFO-MPO/Python') #change directory\n",
    "os.getcwd()  # check where the directory is (and whether the change was successful or not)\n",
    "LOGGING_PATH = \"/metric_output_logs/\" # Where detailed metric outputs are stored if dimension has return_type set to 'dataset'\n",
    "GLOBAL_USER = \"username\" # Username too keep track of who ran tests for output logging\n",
    "DATA_FILE_PATH = \"dataset.xlsx\" # Path to csv or xlsx dataset\n",
    "DIMENSION_SCORES = [] # Stores the final score for each dimension used to calculate final grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 1 (C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is best run on CSV data where the column names are in the first row. It can also accept files that are in xlsx formats but it will only take data from the first sheet if there are more than one sheet in the excel file.\n",
    "\n",
    "Limitations: It will not check for differences in capitalization of the same word (since all the words will be changed to lower case before the similarity score is calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 2 (C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of datasets with a reference list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compared columns in question must be identical to the ref list, otherwise they will be penalized more harshly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mNumber of weights does not match number of metrics run, using default weights instead!\u001b[0m\n",
      "\u001b[31mWeights do not add up to 1.0, using default weights instead!\u001b[0m\n",
      "{'dimension': 'Consistency', 'score': 0.993045537439778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:361: DtypeWarning: Columns (59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(logging_path)\n"
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "    \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "}  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "\n",
    "# Test Consistency Calculations\n",
    "# Using default thresholds and stop words for both metrics\n",
    "consitancy_tests = Consistency(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    c1_column_names=[\"DESCR\"],\n",
    "    c2_column_mapping=column_mapping,\n",
    "    ref_dataset_path=\"Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\",\n",
    "    return_type='dataset', # Can set to score or dataset (if dataset will create 1 sentence summary in output report)\n",
    "    c2_stop_words=['']\n",
    "    # logging_path=LOGGING_PATH # By default this is stored in memory (if set and return_type is dataset, will store csv of all numbers used in metric calculations)\n",
    ")\n",
    "\n",
    "consistancy_score = calculate_dimension_score(\"Consistency\", scores=consitancy_tests.run_metrics([\"C2\"]), weights={})\n",
    "DIMENSION_SCORES.append(consistancy_score)\n",
    "print(consistancy_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 1 (A1, Mixed Data Types, Symbols in Numerics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether there are symbols in numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 2 (A2 Outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find outliers that are 1.5 (or any threshold) times away from the inter-quartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1\n",
      "Column being processed: Escapement_Total\n",
      "Outliers before grouping: SMU                       \n",
      "ALSEK CHINOOK SALMON  3596    False\n",
      "                      3597    False\n",
      "                      3598    False\n",
      "                      3599    False\n",
      "                      3600    False\n",
      "                              ...  \n",
      "YUKON RIVER CHUM      3928    False\n",
      "                      3929    False\n",
      "                      3930    False\n",
      "                      3931    False\n",
      "                      3932    False\n",
      "Name: Escapement_Total, Length: 4223, dtype: bool\n",
      "Outliers index: MultiIndex([('ALSEK CHINOOK SALMON', 3596),\n",
      "            ('ALSEK CHINOOK SALMON', 3597),\n",
      "            ('ALSEK CHINOOK SALMON', 3598),\n",
      "            ('ALSEK CHINOOK SALMON', 3599),\n",
      "            ('ALSEK CHINOOK SALMON', 3600),\n",
      "            ('ALSEK CHINOOK SALMON', 3601),\n",
      "            ('ALSEK CHINOOK SALMON', 3602),\n",
      "            ('ALSEK CHINOOK SALMON', 3603),\n",
      "            ('ALSEK CHINOOK SALMON', 3604),\n",
      "            ('ALSEK CHINOOK SALMON', 3605),\n",
      "            ...\n",
      "            (    'YUKON RIVER CHUM', 3923),\n",
      "            (    'YUKON RIVER CHUM', 3924),\n",
      "            (    'YUKON RIVER CHUM', 3925),\n",
      "            (    'YUKON RIVER CHUM', 3926),\n",
      "            (    'YUKON RIVER CHUM', 3927),\n",
      "            (    'YUKON RIVER CHUM', 3928),\n",
      "            (    'YUKON RIVER CHUM', 3929),\n",
      "            (    'YUKON RIVER CHUM', 3930),\n",
      "            (    'YUKON RIVER CHUM', 3931),\n",
      "            (    'YUKON RIVER CHUM', 3932)],\n",
      "           names=['SMU', None], length=4223)\n",
      "Groupby column: ['SMU']\n",
      "test 2\n",
      "\u001b[31mNumber of weights does not match number of metrics run, using default weights instead!\u001b[0m\n",
      "\u001b[31mWeights do not add up to 1.0, using default weights instead!\u001b[0m\n",
      "{'dimension': 'Accuracy', 'score': 0.9722222222222222}\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy Calculations\n",
    "# Using default threshold, group by, and min score for A2 metric \n",
    "accuracy_tests = Accuracy(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    # selected_columns=[\" Egg Target \", \" Release/ Transfer Target \", \" Coded Wire Tag Target \", \" Fin Clip Target \", \" Thermal Mark Target \", \" Parentage-based Tag Target \", \" PIT Tag Target \"]\n",
    "    selected_columns=[\"Escapement_Total\"],\n",
    "    groupby_column=['SMU'],\n",
    "    return_type='dataset' # Can set to score or dataset (if dataset will create 1 sentence summary in output report)\n",
    "    # logging_path=LOGGING_PATH # By default this is stored in memory (if set and return_type is dataset, will store csv of all numbers used in metric calculations)\n",
    ")\n",
    "\n",
    "accuracy_score = calculate_dimension_score(\"Accuracy\", scores=accuracy_tests.run_metrics([\"A2\"]), weights={})\n",
    "DIMENSION_SCORES.append(accuracy_score)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completeness Type 1 (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is for removing a column that meets the threshold of the percentage of blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mNumber of weights does not match number of metrics run, using default weights instead!\u001b[0m\n",
      "\u001b[31mWeights do not add up to 1.0, using default weights instead!\u001b[0m\n",
      "{'dimension': 'Completeness', 'score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "completeness_tests = Completeness(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    return_type='dataset' # Can set to score or dataset (if dataset will create 1 sentence summary in output report)\n",
    "    # logging_path=LOGGING_PATH # By default this is stored in memory (if set and return_type is dataset, will store csv of all numbers used in metric calculations)\n",
    ")\n",
    "\n",
    "completeness_score = calculate_dimension_score(\"Completeness\", completeness_tests.run_metrics(), weights={})\n",
    "DIMENSION_SCORES.append(completeness_score)\n",
    "print(completeness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness (U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness Type 1 (U1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [QTY_SOLD, DESCRIPTION, Description FR]\n",
      "Index: []\n",
      "\n",
      "Duplication Score: 100.0%\n",
      "When trying to create one line summary for U1, the following error occurred: '>' not supported between instances of 'str' and 'int'\n",
      "\u001b[31mNumber of weights does not match number of metrics run, using default weights instead!\u001b[0m\n",
      "\u001b[31mWeights do not add up to 1.0, using default weights instead!\u001b[0m\n",
      "{'dimension': 'Uniqueness', 'score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "uniqueness_tests = Uniqueness(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    return_type='dataset' # Can set to score or dataset (if dataset will create 1 sentence summary in output report)\n",
    "    # logging_path=LOGGING_PATH # By default this is stored in memory (if set and return_type is dataset, will store csv of all numbers used in metric calculations)\n",
    ")\n",
    "\n",
    "uniqueness_score = calculate_dimension_score(\"Uniqueness\", uniqueness_tests.run_metrics(), weights={})\n",
    "DIMENSION_SCORES.append(uniqueness_score)\n",
    "print(uniqueness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Overall Data Quality Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQ grade for this dataset is: C\n"
     ]
    }
   ],
   "source": [
    "# Call grade calculation here\n",
    "print(f'DQ grade for this dataset is: {calculate_DQ_grade(DIMENSION_SCORES)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
