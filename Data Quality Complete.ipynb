{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run everything in the **Setup** section. \n",
    "    - Make sure to change the working directory to **your** working directory. The code for this is already there.\n",
    "    - Make sure the Excel document for logging the scores also exists in your working directory, and that the file name is correct.\n",
    "\n",
    "2. Determine *if the test needs to be run* by having a good understanding of what each test is doing. \n",
    "    - Please refer to this document [here](https://086gc.sharepoint.com/:x:/r/sites/PacificSalmonTeam/Shared%20Documents/General/02%20-%20PSSI%20Secretariat%20Teams/04%20-%20Strategic%20Salmon%20Data%20Policy%20and%20Analytics/02%20-%20Data%20Governance/00%20-%20Projects/10%20-%20Data%20Quality/Presentation/DQP%20Demo.xlsx?d=wc15abe6743954df980a05f09fe99a560&csf=1&web=1&e=CJeb6h)\n",
    "\n",
    "3. Some requirements for the datasets:\n",
    "    - The data must be on the **first sheet** in the Excel document.\n",
    "    - The **first row** must be the column names. \n",
    "    - The test won't run if the Excel file is open\n",
    "\n",
    "4. After running all the tests, the Excel document for logging the scores can be uploaded to Sharepoint using the function \"Saving the file to sharepoint\". \n",
    "\n",
    "Note: The Output Reports are used for when a data steward is asking about why their dataset gets a certain score. If the metric is not in Output Reports, then running the test itself will generate an output that can be put into a report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run everything in the set up, and double check the working directory so that the data can be read from that same directory.\n",
    "\n",
    "All of these functions are used in the process of calculating data quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set to the correct working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to the same place where you saved the test datasets\n",
    "# os.chdir('C:/Users/luos/OneDrive - DFO-MPO/Python') #change directory\n",
    "os.getcwd()  # check where the directory is (and whether the change was successful or not)\n",
    "GLOBAL_USER = \"EwertM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read either csv or xlsx data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 0: Reading the dataset file\n",
    "def read_data(dataset_path):\n",
    "    _, file_extension = os.path.splitext(dataset_path)\n",
    "    if file_extension == \".csv\":\n",
    "        df = pd.read_csv(\n",
    "            dataset_path, encoding=\"cp1252\"\n",
    "        )  # sometimes if the function has issue reading a csv file, include: , encoding = 'cp1252')\n",
    "    elif file_extension == \".xlsx\":\n",
    "        df = pd.read_excel(dataset_path)\n",
    "    else:\n",
    "        print(\"Unsupported file type\")\n",
    "        df = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to log the scores into an xlsx file (already created, existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log a new row into the DQS_Log_XX.xlsx file\n",
    "def log_score(test_name, dataset_name, score, selected_columns, threshold=None):\n",
    "    # Convert score to a percentage\n",
    "    percentage_score = score\n",
    "\n",
    "    # Load the Excel file into a DataFrame\n",
    "    log_file = \"DQS_Log_Beta.xlsx\"\n",
    "\n",
    "    # Set threshold to \"No threshold\" if it is not provided\n",
    "    if threshold is None:\n",
    "        threshold_value = \"no threshold\"\n",
    "    else:\n",
    "        threshold_value = threshold\n",
    "\n",
    "    # If selected_columns is None, assume \"All\" was tested\n",
    "    if selected_columns is None:\n",
    "        columns_tested = \"All columns\"\n",
    "    else:\n",
    "        # Convert selected_columns list to a string if specific columns are provided\n",
    "        columns_tested = \", \".join(selected_columns)\n",
    "\n",
    "    # Try loading the existing Excel file\n",
    "    try:\n",
    "        df = read_data(log_file)\n",
    "    except FileNotFoundError:\n",
    "        # Create an empty DataFrame if file doesn't exist (shouldn't be the case if you already created it)\n",
    "        df = pd.DataFrame(\n",
    "            columns=[\"Dataset\", \"Test\", \"Threshold\", \"Date_Calculated\", \"Score\"]\n",
    "        )\n",
    "\n",
    "    # Prepare the new row as a DataFrame\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Dataset\": [dataset_name],\n",
    "            \"Columns_Tested\": [columns_tested],  # Add the list of columns tested\n",
    "            \"Test\": [test_name],\n",
    "            \"Date_Calculated\": [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"Threshold\": [threshold_value],\n",
    "            \"Score\": [percentage_score],\n",
    "            \"User\": GLOBAL_USER\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(log_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract dataset name from a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(dataset_path):\n",
    "    # Extract the file name from the path (e.g., 'Dataset_A.csv')\n",
    "    file_name = os.path.basename(dataset_path)\n",
    "    # Split the file name to remove the extension (e.g., 'Dataset_A')\n",
    "    dataset_name = os.path.splitext(file_name)[0]\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 1 (C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is best run on CSV data where the column names are in the first row. It can also accept files that are in xlsx formats but it will only take data from the first sheet if there are more than one sheet in the excel file.\n",
    "\n",
    "Limitations: It will not check for differences in capitalization of the same word (since all the words will be changed to lower case before the similarity score is calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency Type 1 (C1) function\n",
    "\n",
    "# Dictionary mapping Canadian province abbreviations to their full names\n",
    "province_abbreviations = {\n",
    "    \"BC\": \"British Columbia\",\n",
    "    \"ON\": \"Ontario\",\n",
    "    \"QC\": \"Quebec\",\n",
    "    \"AB\": \"Alberta\",\n",
    "    \"MB\": \"Manitoba\",\n",
    "    \"SK\": \"Saskatchewan\",\n",
    "    \"NS\": \"Nova Scotia\",\n",
    "    \"NB\": \"New Brunswick\",\n",
    "    \"NL\": \"Newfoundland and Labrador\",\n",
    "    \"PE\": \"Prince Edward Island\",\n",
    "    \"NT\": \"Northwest Territories\",\n",
    "    \"YT\": \"Yukon\",\n",
    "    \"NU\": \"Nunavut\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_text(text, remove_numbers=False):\n",
    "    \"\"\"\n",
    "    Normalize input text by converting to lowercase, stripping whitespace,\n",
    "    replacing province abbreviations with full names, and removing non-alphanumeric characters.\n",
    "    Optionally remove numbers based on the flag.\n",
    "    \"\"\"\n",
    "    text = str(text).lower().strip()\n",
    "    for abbr, full in province_abbreviations.items():\n",
    "        text = re.sub(r\"\\b\" + abbr.lower() + r\"\\b\", full.lower(), text)\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = \"\".join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def extract_numbers(text):\n",
    "    \"\"\"\n",
    "    Extract all numbers from the input text and return them as a list of strings.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\d+\", text)\n",
    "\n",
    "\n",
    "def remove_short_numbers(text):\n",
    "    \"\"\"\n",
    "    Remove numbers with 1 or 2 digits from the input text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\b\\d{1,4}\\b\", \"\", text)\n",
    "\n",
    "\n",
    "def numeric_similarity(num1_list, num2_list):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two lists of numbers by comparing each digit.\n",
    "    Return the proportion of matching digits.\n",
    "    \"\"\"\n",
    "    num1, num2 = \" \".join(num1_list), \" \".join(num2_list)\n",
    "    matches = sum(1 for a, b in zip(num1, num2) if a == b)\n",
    "    max_length = max(len(num1), len(num2))\n",
    "    return matches / max_length if max_length > 0 else 0\n",
    "\n",
    "\n",
    "def string_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two strings using the SequenceMatcher from difflib.\n",
    "    Return the similarity ratio.\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text_list, ref_list, Stop_Words):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between lists of texts using TF-IDF vectorization.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=Stop_Words, analyzer=\"word\", ngram_range=(1, 2)\n",
    "    )\n",
    "    ref_vec = vectorizer.fit_transform(ref_list)\n",
    "    text_vec = vectorizer.transform(text_list)\n",
    "    return cosine_similarity(text_vec, ref_vec)\n",
    "\n",
    "\n",
    "def contains_short_number(num_list):\n",
    "    \"\"\"\n",
    "    Check if any number in the list has 1 or 2 digits.\n",
    "    \"\"\"\n",
    "    return any(len(num) <= 4 for num in num_list)\n",
    "\n",
    "\n",
    "def numbers_match(num_list1, num_list2):\n",
    "    \"\"\"\n",
    "    Check if any number in the first list is present in the second list.\n",
    "    \"\"\"\n",
    "    return any(num in num_list2 for num in num_list1)\n",
    "\n",
    "\n",
    "def calculate_combined_similarity(df, unique_observations, text_similarity_matrix):\n",
    "    \"\"\"\n",
    "    Combine text and numeric similarities into a single similarity matrix.\n",
    "    \"\"\"\n",
    "    # Make a copy of the text similarity matrix to modify it\n",
    "    combined_sim_matrix = np.copy(text_similarity_matrix)\n",
    "\n",
    "    # Extract numeric parts from each unique observation\n",
    "    numeric_parts = [extract_numbers(obs) for obs in unique_observations]\n",
    "\n",
    "    # Iterate over each pair of unique observations to calculate numeric similarity\n",
    "    for i, num_i in enumerate(numeric_parts):\n",
    "        for j, num_j in enumerate(numeric_parts):\n",
    "            if i != j:\n",
    "                # Calculate the numeric similarity for the current pair\n",
    "                num_sim = numeric_similarity(num_i, num_j)\n",
    "\n",
    "                # Update the combined similarity matrix with the maximum value between text and numeric similarity\n",
    "                combined_sim_matrix[i, j] = max(combined_sim_matrix[i, j], num_sim)\n",
    "\n",
    "    # Iterate over each pair of unique observations to calculate string similarity\n",
    "    for i, obs_i in enumerate(unique_observations):\n",
    "        for j, obs_j in enumerate(unique_observations):\n",
    "            if i != j:\n",
    "                # Calculate the string similarity for the current pair\n",
    "                seq_sim = string_similarity(obs_i, obs_j)\n",
    "\n",
    "                # Update the combined similarity matrix with the maximum value between existing and sequence matcher\n",
    "                combined_sim_matrix[i, j] = max(combined_sim_matrix[i, j], seq_sim)\n",
    "\n",
    "    return combined_sim_matrix\n",
    "\n",
    "\n",
    "def average_consistency_score(cosine_sim_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the average consistency score based on the cosine similarity matrix and a given threshold.\n",
    "    \"\"\"\n",
    "    num_rows, num_columns = cosine_sim_matrix.shape\n",
    "    inconsistency = 0\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        if np.any(\n",
    "            (cosine_sim_matrix[i] > threshold) & (cosine_sim_matrix[i] <= 1.0000000)\n",
    "        ):\n",
    "            inconsistency += 1\n",
    "\n",
    "    return (num_rows - inconsistency) / num_rows\n",
    "\n",
    "\n",
    "def process_and_calculate_similarity(\n",
    "    dataset_path, column_names, threshold, Stop_Words=[\"the\", \"and\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the dataset, normalize the text, and calculate the similarity scores for multiple columns.\n",
    "    \"\"\"\n",
    "    # Read the dataset from the provided Excel file path\n",
    "    df = read_data(dataset_path)\n",
    "    overall_consistency_scores = []\n",
    "\n",
    "    # Iterate over each specified column\n",
    "    for column_name in column_names:\n",
    "        # Normalize the text in the specified column and store the results in a new column\n",
    "        df[f\"Normalized {column_name}\"] = df[column_name].apply(normalize_text)\n",
    "\n",
    "        # Get unique normalized observations by removing duplicates and NaN values\n",
    "        unique_observations = pd.unique(\n",
    "            df[f\"Normalized {column_name}\"].dropna().values.ravel()\n",
    "        )\n",
    "\n",
    "        # Calculate the cosine similarity matrix for the unique normalized observations\n",
    "        text_sim_matrix = calculate_cosine_similarity(\n",
    "            unique_observations.tolist(), unique_observations.tolist(), Stop_Words\n",
    "        )\n",
    "\n",
    "        # Set the diagonal of the similarity matrix to 0 to ignore self-similarity\n",
    "        np.fill_diagonal(text_sim_matrix, 0)\n",
    "\n",
    "        # Combine text similarity with numeric similarity to get a final similarity matrix\n",
    "        combined_sim_matrix = calculate_combined_similarity(\n",
    "            df, unique_observations, text_sim_matrix\n",
    "        )\n",
    "\n",
    "        # Initialize columns in the dataframe to store the recommended organization matches and all matches\n",
    "        df[f\"Recommended {column_name}\"] = None\n",
    "        df[f\"All Matches {column_name}\"] = None\n",
    "\n",
    "        # Iterate over each normalized organization in the dataframe\n",
    "        for i, norm_org in enumerate(df[f\"Normalized {column_name}\"]):\n",
    "            # Find the index of the current normalized organization in the unique observations\n",
    "            try:\n",
    "                current_index = np.where(unique_observations == norm_org)[0][0]\n",
    "            except IndexError:\n",
    "                df.at[i, f\"Recommended {column_name}\"] = \"No significant match\"\n",
    "                df.at[i, f\"All Matches {column_name}\"] = []\n",
    "                continue\n",
    "\n",
    "            # Get the similarities for the current organization from the combined similarity matrix\n",
    "            similarities = combined_sim_matrix[current_index]\n",
    "\n",
    "            # Find the indices and values of all matching organizations\n",
    "            matched_indices = np.where(similarities >= threshold)[0]\n",
    "            all_matches = [unique_observations[idx] for idx in matched_indices]\n",
    "            all_match_scores = [similarities[idx] for idx in matched_indices]\n",
    "\n",
    "            best_score = 0\n",
    "            best_match = \"No significant match\"\n",
    "\n",
    "            # Extract numbers from the current organization\n",
    "            num_list_current = extract_numbers(norm_org)\n",
    "\n",
    "            for idx in matched_indices:\n",
    "                candidate_match = unique_observations[idx]\n",
    "                num_list_candidate = extract_numbers(candidate_match)\n",
    "\n",
    "                if contains_short_number(num_list_current) or contains_short_number(\n",
    "                    num_list_candidate\n",
    "                ):\n",
    "                    # If short numbers are present, ensure they match; otherwise, skip this match\n",
    "                    if not numbers_match(num_list_current, num_list_candidate):\n",
    "                        continue\n",
    "                    # Recalculate similarity excluding short numbers\n",
    "                    norm_org_no_nums = remove_short_numbers(norm_org)\n",
    "                    candidate_no_nums = remove_short_numbers(candidate_match)\n",
    "                    recalculated_similarity = string_similarity(\n",
    "                        norm_org_no_nums, candidate_no_nums\n",
    "                    )\n",
    "                    if recalculated_similarity > best_score:\n",
    "                        best_score = recalculated_similarity\n",
    "                        best_match = candidate_match\n",
    "                else:\n",
    "                    if similarities[idx] > best_score:\n",
    "                        best_score = similarities[idx]\n",
    "                        best_match = candidate_match\n",
    "\n",
    "            # Assign the best match to the dataframe\n",
    "            if best_score > threshold:\n",
    "                df.at[i, f\"Recommended {column_name}\"] = (\n",
    "                    f\"{best_match} ({best_score:.2f})\"\n",
    "                )\n",
    "            else:\n",
    "                df.at[i, f\"Recommended {column_name}\"] = \"No significant match\"\n",
    "\n",
    "            # Store all matches\n",
    "            df.at[i, f\"All Matches {column_name}\"] = \", \".join(\n",
    "                [\n",
    "                    f\"{match} ({score:.2f})\"\n",
    "                    for match, score in zip(all_matches, all_match_scores)\n",
    "                    if score > threshold\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Calculate the overall consistency score for the current column\n",
    "        consistency_score = average_consistency_score(text_sim_matrix, threshold)\n",
    "        overall_consistency_scores.append(consistency_score)\n",
    "\n",
    "    # Calculate the overall consistency score as the average of individual consistency scores\n",
    "    overall_consistency_score = np.mean(overall_consistency_scores)\n",
    "    df[\"Overall Consistency Score\"] = overall_consistency_score\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Consistency (C1)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=column_names,\n",
    "        threshold=threshold,\n",
    "        score=overall_consistency_score,\n",
    "    )\n",
    "\n",
    "    return overall_consistency_score  # to return the score\n",
    "    # return df #to return the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"Salmon Head Depot\"\n",
    "datafile = \"Pacific-Recreational-Fishery-Salmon-Head-Depots.csv\"\n",
    "datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{dataset}/{datafile}\"\n",
    "# Test Consistency Calculations\n",
    "\n",
    "processed_df = process_and_calculate_similarity(\n",
    "    dataset_path=datafilepath,\n",
    "    column_names=[\n",
    "        \"DEPOT NAME / NOM DU DÉPÔT\",\n",
    "        \"AREA / LA RÉGION\",\n",
    "        \"MUNICIPALITY / MUNICIPALITÉ\",\n",
    "        \"ADDRESS / ADRESSE\",\n",
    "        \"STORAGE INFORMATION / DÉTAILS DE STOCKAGE\",\n",
    "    ],\n",
    "    threshold=0.91,\n",
    ")\n",
    "\n",
    "# processed_df['Overall Consistency Score'].min()\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 2 (C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of datasets with a reference list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compared columns in question must be identical to the ref list, otherwise they will be penalized more harshly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Get names used for a single column\n",
    "def get_names_used_for_column(df, column_name):\n",
    "    unique_observations = pd.unique(df[column_name].dropna().values.ravel())\n",
    "    return unique_observations\n",
    "\n",
    "\n",
    "# Function 2: Calculate Cosine Similarity\n",
    "def calculate_cosine_similarity(text_list, ref_list, Stop_Words):\n",
    "    count_vectorizer = CountVectorizer(stop_words=Stop_Words)\n",
    "    ref_vec = count_vectorizer.fit_transform(ref_list).todense()\n",
    "    ref_vec_array = np.array(ref_vec)\n",
    "    text_vec = count_vectorizer.transform(text_list).todense()\n",
    "    text_vec_array = np.array(text_vec)\n",
    "    cosine_sim = np.round((cosine_similarity(text_vec_array, ref_vec_array)), 2)\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "# Function 3: Average Consistency Score\n",
    "def average_consistency_score(cosine_sim_df, threshold=0.91):\n",
    "    num_rows, num_columns = cosine_sim_df.shape\n",
    "    total_count = 0  # This will count all values above or equal to the threshold\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        if np.max(cosine_sim_df[i]) >= threshold:  # Include all comparisons\n",
    "            total_count += 1\n",
    "    total_observations = num_rows  # Total number of observations\n",
    "    average_consistency_score = total_count / total_observations\n",
    "    return average_consistency_score\n",
    "\n",
    "\n",
    "def process_and_calculate_similarity_ref(\n",
    "    dataset_path,\n",
    "    column_mapping,\n",
    "    ref_dataset_path=None,\n",
    "    threshold=0.91,\n",
    "    Stop_Words=\"activity\",\n",
    "):\n",
    "    # Read the data file\n",
    "    df = read_data(dataset_path)\n",
    "\n",
    "    # Initialize ref_df if a ref dataset is provided\n",
    "    if ref_dataset_path:\n",
    "        df_ref = read_data(ref_dataset_path)\n",
    "        ref_data = True  # Flag to indicate we are using a ref dataset\n",
    "    else:\n",
    "        ref_data = False  # No ref dataset, compare within the same dataset\n",
    "\n",
    "    all_consistency_scores = []\n",
    "\n",
    "    for selected_column, m_selected_column in column_mapping.items():\n",
    "        if ref_data:\n",
    "            # Compare to ref dataset\n",
    "            unique_observations = get_names_used_for_column(df_ref, m_selected_column)\n",
    "        else:\n",
    "            # Use own column for comparison\n",
    "            unique_observations = get_names_used_for_column(df, selected_column)\n",
    "\n",
    "        cosine_sim_matrix = calculate_cosine_similarity(\n",
    "            df[selected_column].dropna(), unique_observations, Stop_Words=Stop_Words\n",
    "        )\n",
    "        column_consistency_score = average_consistency_score(\n",
    "            cosine_sim_matrix, threshold\n",
    "        )\n",
    "        all_consistency_scores.append(column_consistency_score)\n",
    "\n",
    "    # Calculate the average of all consistency scores\n",
    "    overall_avg_consistency = (\n",
    "        sum(all_consistency_scores) / len(all_consistency_scores)\n",
    "        if all_consistency_scores\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Consistency (C2)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=column_mapping,\n",
    "        threshold=threshold,\n",
    "        score=overall_avg_consistency,\n",
    "    )\n",
    "\n",
    "    print(f\"overall_avg_consistency = {overall_avg_consistency}\")\n",
    "    return overall_avg_consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_mapping = {\n",
    "#     \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "#     \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "# }  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "# process_and_calculate_similarity_ref(\n",
    "#     dataset_path=\"data/test/2024-03-28 1_qryThermal_NatEmerg.xlsx\",\n",
    "#     column_mapping=column_mapping,\n",
    "#     ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\",\n",
    "#     threshold=1,\n",
    "#     Stop_Words=[\"\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 1 (A1, Mixed Data Types, Symbols in Numerics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether there are symbols in numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Using isdigit to find non-numerical entries\n",
    "def find_non_digits(s):\n",
    "    # Ensure the value is treated as a string\n",
    "    s = str(s)\n",
    "    return [char for char in s if not (char.isdigit() or char == \".\")]\n",
    "\n",
    "\n",
    "# Function 2 : Calculate the score\n",
    "def accuracy_score(dataset_path, selected_columns):\n",
    "    adf = read_data(dataset_path)\n",
    "    selected_columns = [col for col in adf.columns if col in selected_columns]\n",
    "\n",
    "    all_accuracy_scores = []\n",
    "\n",
    "    for column_name in selected_columns:\n",
    "        # Drop NA, null, or blank values from column\n",
    "        column_data = adf[column_name].dropna()\n",
    "\n",
    "        total_rows = len(column_data)\n",
    "\n",
    "        if total_rows > 0:  # to avoid division by zero\n",
    "            non_digit_chars_per_row = column_data.apply(find_non_digits)\n",
    "            non_numerical_count = non_digit_chars_per_row.apply(\n",
    "                lambda x: len(x) > 0\n",
    "            ).sum()\n",
    "            accuracy_score = (total_rows - non_numerical_count) / total_rows\n",
    "            all_accuracy_scores.append(accuracy_score)\n",
    "\n",
    "    overall_accuracy_score = (\n",
    "        sum(all_accuracy_scores) / len(all_accuracy_scores)\n",
    "        if all_accuracy_scores\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Accuracy (A1)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=selected_columns,\n",
    "        threshold=None,\n",
    "        score=overall_accuracy_score,\n",
    "    )\n",
    "    print(f\"overall_avg_consistency = {overall_accuracy_score}\")\n",
    "    return overall_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_avg_consistency = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.5)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = \"NuSEDS Escapement\"\n",
    "# datafile = \"Yukon and Transboundary NuSEDS_20241004.xlsx\"\n",
    "# datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{dataset}/{datafile}\"\n",
    "accuracy_score(\n",
    "    dataset_path=datafilepath,\n",
    "    selected_columns=[\"LATITUDE / LATITUDE\", \"LONGITUDE / LONGITUDE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 2 (A2 Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_iqr(\n",
    "    dataset_path,\n",
    "    selected_columns,\n",
    "    groupby_column=None,\n",
    "    threshold=1.5,\n",
    "    minimum_score=0.85,\n",
    "):\n",
    "    df = read_data(dataset_path)\n",
    "\n",
    "    outliers_dict = {}\n",
    "\n",
    "    # If a groupby column is specified, perform the IQR calculation within each group\n",
    "\n",
    "    if groupby_column:\n",
    "        grouped = df.groupby(groupby_column)\n",
    "        for column in selected_columns:\n",
    "            # Apply the outlier detection for each group\n",
    "            outliers = grouped[column].apply(\n",
    "                lambda x: (\n",
    "                    (\n",
    "                        x\n",
    "                        < x.quantile(0.25)\n",
    "                        - threshold * (x.quantile(0.75) - x.quantile(0.25))\n",
    "                    )\n",
    "                    | (\n",
    "                        x\n",
    "                        > x.quantile(0.75)\n",
    "                        + threshold * (x.quantile(0.75) - x.quantile(0.25))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            # Combine the outlier Series into a single Series that corresponds to the original DataFrame index\n",
    "            outliers_dict[column] = 1 - outliers.groupby(groupby_column).mean()\n",
    "    else:\n",
    "        # Perform the IQR calculation on the whole column if no groupby column is specified\n",
    "        for column in selected_columns:\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "            outliers_dict[column] = 1 - outliers.mean()\n",
    "\n",
    "    #compute final score\n",
    "    total_groups = len(outliers_dict)\n",
    "    groups_above = sum(1 for score in outliers_dict.values() if score > minimum_score)\n",
    "    final_score = groups_above / total_groups if total_groups > 0 else 0\n",
    "\n",
    "    #final_score = {}\n",
    "\n",
    "    # for key in outliers_dict.keys():\n",
    "    #     print(outliers_dict[key])\n",
    "    #     arr = outliers_dict[key].values\n",
    "    #     value_out = np.sum(arr > minimum_score) / len(arr)\n",
    "    #     final_score[key] = value_out\n",
    "    \n",
    "    # for key, value in outliers_dict.items():  \n",
    "    #     print(key, value)\n",
    "    #     # Check if the proportion of non-outliers is greater than the minimum score  \n",
    "    #     value_out = value > minimum_score  \n",
    "    #     # Store the result (True or False) in the final_score dictionary  \n",
    "    #     final_score[key] = value_out  \n",
    "\n",
    "    # log the results\n",
    "\n",
    "    log_score(\n",
    "        test_name=\"Accuracy (A2)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=selected_columns,\n",
    "        threshold=threshold,\n",
    "        score=final_score,\n",
    "    )\n",
    "\n",
    "    return outliers_dict, final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'LATITUDE / LATITUDE': np.float64(0.8693693693693694),\n",
       "  'LONGITUDE / LONGITUDE': np.float64(0.9819819819819819)},\n",
       " 1.0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = \"NuSEDS Escapement\"\n",
    "# datafile = \"Johnstone Strait and Strait of Georgia NuSEDS_20241004.xlsx\"\n",
    "# datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{dataset}/{datafile}\"\n",
    "find_outliers_iqr(\n",
    "    dataset_path=datafilepath,\n",
    "    selected_columns=[\"LATITUDE / LATITUDE\", \"LONGITUDE / LONGITUDE\"],\n",
    "    threshold=1.5,\n",
    "    minimum_score=0.85,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 3 (A3 Duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 1: finding duplicates\n",
    "def find_duplicates_and_percentage(dataset_path):\n",
    "\n",
    "    df = read_data(dataset_path)\n",
    "\n",
    "    # Find duplicate rows\n",
    "    duplicate_rows = df[df.duplicated(keep=False)]\n",
    "\n",
    "    # Calculate percentage of duplicate rows\n",
    "    total_rows = len(df)\n",
    "    total_duplicate_rows = len(duplicate_rows)\n",
    "    percentage_duplicate = 1 - (total_duplicate_rows / total_rows)\n",
    "\n",
    "    # Print duplicate rows\n",
    "    print(\"Duplicate Rows:\")\n",
    "    print(duplicate_rows)\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Accuracy (A3)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=None,\n",
    "        threshold=None,\n",
    "        score=percentage_duplicate,\n",
    "    )\n",
    "\n",
    "    # Print percentage of duplicate rows\n",
    "    print(f\"\\nDuplication Score: {percentage_duplicate*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [DEPOT NAME / NOM DU DÉPÔT, AREA / LA RÉGION, MUNICIPALITY / MUNICIPALITÉ, ADDRESS / ADRESSE, PHONE NUMBER / NUMÉRO DE TÉLÉPHONE, ACCESSIBILITY / ACCESSIBILITÉ, STORAGE INFORMATION / DÉTAILS DE STOCKAGE, LATITUDE / LATITUDE, LONGITUDE / LONGITUDE]\n",
      "Index: []\n",
      "\n",
      "Duplication Score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# dataset = \"NuSEDS Escapement\"\n",
    "# datafile = \"Yukon and Transboundary NuSEDS_20241004.xlsx\"\n",
    "# datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{dataset}/{datafile}\"\n",
    "find_duplicates_and_percentage(\n",
    "    dataset_path=datafilepath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is for removing a column that meets the threshold of the percentage of blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_test(dataset_path, exclude_columns=[], threshold=0.75):\n",
    "    dataset = read_data(dataset_path)\n",
    "\n",
    "    # Exclude the 'Comment' column if it exists in the dataset\n",
    "    if \"Comment\" in dataset.columns:\n",
    "        dataset = dataset.drop(columns=[\"Comment\"])\n",
    "\n",
    "    # Exclude columns in exclude_columns if they exist in the dataset\n",
    "    dataset = dataset.drop(\n",
    "        columns=[col for col in exclude_columns if col in dataset.columns]\n",
    "    )\n",
    "\n",
    "    # Calculate the percentage of non-null (non-missing) values in each column\n",
    "    is_null_percentage = dataset.isna().mean()\n",
    "\n",
    "    # Identify columns with non-null percentage less than or equal to the threshold\n",
    "    columns_to_keep = is_null_percentage[is_null_percentage <= threshold].index\n",
    "\n",
    "    # Keep columns that exceed the threshold of non-null values\n",
    "    dataset2 = dataset[columns_to_keep]\n",
    "\n",
    "    # Calculate the actual percentage of non-missing values in the dataset\n",
    "    total_non_missing = dataset2.notna().sum().sum()\n",
    "    total_obs = dataset2.shape[0] * dataset2.shape[1]\n",
    "    completeness_score = total_non_missing / total_obs\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Completeness (P)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=None,\n",
    "        threshold=threshold,\n",
    "        score=completeness_score,\n",
    "    )\n",
    "\n",
    "    return completeness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9764764764764765)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"North and Central Coast NuSEDS_20241004.xlsx\"\n",
    "# \"West Coast Vancouver Island NuSEDS_20241004.xlsx\"\n",
    "# \"Yukon and Transboundary NuSEDS_20241004.xlsx\"\n",
    "# dataset = \"NuSEDS Escapement\"\n",
    "# datafile = \"Johnstone Strait and Strait of Georgia NuSEDS_20241004.xlsx\"\n",
    "# datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{dataset}/{datafile}\"\n",
    "completeness_test(\n",
    "    datafilepath,\n",
    "    threshold=0.75,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def calc_timeliness(refresh_date, cycle_day):\n",
    "    refresh_date = pd.to_datetime(refresh_date)\n",
    "    unupdate_cycle = np.max([((datetime.now() - refresh_date).days / cycle_day) - 1, 0])\n",
    "\n",
    "    # unupdate_cycle = np.floor((datetime.now() - refresh_date).days/cycle_day)\n",
    "    # print((datetime.now() - refresh_date).days/cycle_day)\n",
    "    return np.max([0, 100 - (unupdate_cycle * (100 / 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(66.48401826484017)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_timeliness(\"2022-12-01\", cycle_day=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Reports\n",
    "Run all the functions above first before running this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that output reports can be generated through the data quality tests of\n",
    "<p>    - Consistency type 1\n",
    "<p>    - Accuracy type 2\n",
    "<p>    - Accuracy type 3\n",
    "<p>    - Completeness\n",
    "<p>          \n",
    "<p>  *Completeness test does not require an output report (just find the blanks in the dataset). The rest can be found below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_datasets(dataset_path, column_mapping, ref_dataset_path=None):\n",
    "    # Read the data file\n",
    "    df = read_data(dataset_path)\n",
    "\n",
    "    # Initialize ref_df if a ref dataset is provided\n",
    "    if ref_dataset_path:\n",
    "        df_ref = read_data(ref_dataset_path)\n",
    "        ref_data = True  # Flag to indicate we are using a ref dataset\n",
    "    else:\n",
    "        ref_data = False  # No ref dataset, compare within the same dataset\n",
    "\n",
    "    for selected_column, m_selected_column in column_mapping.items():\n",
    "        if ref_data:\n",
    "            # Compare to ref dataset\n",
    "            unique_observations = get_names_used_for_column(df_ref, m_selected_column)\n",
    "        else:\n",
    "            # Use own column for comparison\n",
    "            unique_observations = get_names_used_for_column(df, selected_column)\n",
    "\n",
    "        # Iterate over each row in the selected column\n",
    "        column_results = []\n",
    "        for value in df[selected_column]:\n",
    "            # Check if the value exists in unique_observations and append the result to column_results\n",
    "            if pd.isnull(value):\n",
    "                column_results.append(\n",
    "                    False\n",
    "                )  # or True, depending on how you want to handle NaN values\n",
    "            else:\n",
    "                column_results.append(value in unique_observations)\n",
    "\n",
    "        # Add the results as a new column in the DataFrame\n",
    "        df[selected_column + \"_comparison\"] = column_results\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/test/Salmonid_Enhancement_Program_Releases.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m column_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOCK_CU_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCU_Display\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOCK_CU_INDEX\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFULL_CU_IN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m }  \u001b[38;5;66;03m# the pattern for comparison is 'dataset column' : 'reference column'\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcompare_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/test/Salmonid_Enhancement_Program_Releases.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_dataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[125], line 3\u001b[0m, in \u001b[0;36mcompare_datasets\u001b[1;34m(dataset_path, column_mapping, ref_dataset_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare_datasets\u001b[39m(dataset_path, column_mapping, ref_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Read the data file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Initialize ref_df if a ref dataset is provided\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ref_dataset_path:\n",
      "Cell \u001b[1;32mIn[108], line 9\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m(dataset_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m      6\u001b[0m         dataset_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp1252\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m     )  \u001b[38;5;66;03m# sometimes if the function has issue reading a csv file, include: , encoding = 'cp1252')\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_extension \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported file type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/test/Salmonid_Enhancement_Program_Releases.xlsx'"
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "    \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "}  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "compare_datasets(\n",
    "    dataset_path=\"data/test/Salmonid_Enhancement_Program_Releases.xlsx\",\n",
    "    column_mapping=column_mapping,\n",
    "    ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Using isdigit to find non-numerical entries\n",
    "def find_non_digits(s):\n",
    "    # Ensure the value is treated as a string\n",
    "    s = str(s)\n",
    "    return [char for char in s if not (char.isdigit() or char == \".\")]\n",
    "\n",
    "\n",
    "# Function 2 : Check if each row has only numbers in each selected column and add results as new columns\n",
    "def add_only_numbers_columns(dataset_path, selected_columns):\n",
    "    adf = read_data(dataset_path)\n",
    "    selected_columns = [col for col in adf.columns if col in selected_columns]\n",
    "\n",
    "    for column_name in selected_columns:\n",
    "        adf[column_name + \"_Only_Numbers\"] = adf[column_name].apply(\n",
    "            lambda x: len(find_non_digits(x)) == 0\n",
    "        )\n",
    "\n",
    "    return adf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_only_numbers_columns(\n",
    "    dataset_path=\"data/test/SEP Facilities.xlsx\", selected_columns=[\"LicNo\", \"FRN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-11-28 16:53:40'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(dataset_path):\n",
    "    # Extract the file name from the path (e.g., 'Dataset_A.csv')\n",
    "    file_name = os.path.basename(dataset_path)\n",
    "    # Split the file name to remove the extension (e.g., 'Dataset_A')\n",
    "    dataset_name = os.path.splitext(file_name)[0]\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More up to date code for the score log can be found in the \"Setup\" section, the code here is treated more as a testing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log a new row into the DQS_Log.xlsx file\n",
    "def log_score(test_name, dataset_name, score, threshold=None):\n",
    "    # Convert score to a percentage\n",
    "    percentage_score = score * 100\n",
    "\n",
    "    # Load the Excel file into a DataFrame\n",
    "    log_file = \"DQS_Log_Test.xlsx\"\n",
    "\n",
    "    # Set threshold to \"No threshold\" if it is not provided\n",
    "    if threshold is None:\n",
    "        threshold_value = \"no threshold\"\n",
    "    else:\n",
    "        threshold_value = threshold\n",
    "    # Try loading the existing Excel file\n",
    "    try:\n",
    "        df = read_data(log_file)\n",
    "    except FileNotFoundError:\n",
    "        # Create an empty DataFrame if file doesn't exist (shouldn't be the case if you already created it)\n",
    "        df = pd.DataFrame(\n",
    "            columns=[\"Dataset\", \"Test\", \"Threshold\", \"Date_Calculated\", \"Score\"]\n",
    "        )\n",
    "\n",
    "    # Prepare the new row as a DataFrame\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Dataset\": [dataset_name],\n",
    "            \"Test\": [test_name],\n",
    "            \"Date_Calculated\": [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"Threshold\": [threshold_value],\n",
    "            \"Score\": [percentage_score],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(log_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the file to sharepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to copy the log file to another folder\n",
    "def copy_log_file(destination_folder):\n",
    "    # Define the name of the file and the current working directory\n",
    "    log_file = \"DQS_Log_Test.xlsx\"\n",
    "\n",
    "    # Get the current working directory (if needed)\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Define the source path (current working directory + file)\n",
    "    source_path = os.path.join(current_directory, log_file)\n",
    "\n",
    "    # Define the destination path (destination folder + file)\n",
    "    destination_path = os.path.join(destination_folder, log_file)\n",
    "\n",
    "    # Copy the file to the destination folder\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print(f\"File copied to {destination_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function when saving the excel document from the working directory to Sharepoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the destination folder where you want to copy the file\n",
    "destination_folder = \"C:/Users/EwertM/Documents/Portal/DataQuality\"\n",
    "\n",
    "copy_log_file(destination_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
