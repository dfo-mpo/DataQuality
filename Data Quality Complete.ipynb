{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run everything in the **Setup** section. \n",
    "    - Make sure to change the working directory to **your** working directory. The code for this is already there.\n",
    "    - Make sure the Excel document for logging the scores also exists in your working directory, and that the file name is correct.\n",
    "\n",
    "2. Determine *if the test needs to be run* by having a good understanding of what each test is doing. \n",
    "    - Please refer to this document [here](https://086gc.sharepoint.com/:x:/r/sites/PacificSalmonTeam/Shared%20Documents/General/02%20-%20PSSI%20Secretariat%20Teams/04%20-%20Strategic%20Salmon%20Data%20Policy%20and%20Analytics/02%20-%20Data%20Governance/00%20-%20Projects/10%20-%20Data%20Quality/Presentation/DQP%20Demo.xlsx?d=wc15abe6743954df980a05f09fe99a560&csf=1&web=1&e=CJeb6h)\n",
    "\n",
    "3. Some requirements for the datasets:\n",
    "    - The data must be on the **first sheet** in the Excel document.\n",
    "    - The **first row** must be the column names. \n",
    "    - The test won't run if the Excel file is open\n",
    "\n",
    "4. After running all the tests, the Excel document for logging the scores can be uploaded to Sharepoint using the function \"Saving the file to sharepoint\". \n",
    "\n",
    "Note: The Output Reports are used for when a data steward is asking about why their dataset gets a certain score. If the metric is not in Output Reports, then running the test itself will generate an output that can be put into a report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run everything in the set up, and double check the working directory so that the data can be read from that same directory.\n",
    "\n",
    "All of these functions are used in the process of calculating data quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onakd\\AppData\\Local\\Temp\\1\\ipykernel_28156\\1945100915.py:4: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Clear memory\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import nbformat\n",
    "import gc\n",
    "\n",
    "# Import dimentions\n",
    "from dimensions.consistency import Consistency\n",
    "from dimensions.accuracy import Accuracy\n",
    "from dimensions.completeness import Completeness\n",
    "from dimensions.uniqueness import Uniqueness\n",
    "from dimensions.utils import calculate_dimension_score, calculate_DQ_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set to the correct working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to the same place where you saved the test datasets\n",
    "# os.chdir('C:/Users/luos/OneDrive - DFO-MPO/Python') #change directory\n",
    "os.getcwd()  # check where the directory is (and whether the change was successful or not)\n",
    "LOGGING_PATH = \"/metric_output_logs/\"\n",
    "GLOBAL_USER = \"OnakD\"\n",
    "GLOBAL_DATASET = \"NuSEDS Escapement\"\n",
    "GLOBAL_DATAFILE = \"Johnstone Strait and Strait of Georgia NuSEDS_20241004.xlsx\"\n",
    "DATA_FILE_PATH = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{GLOBAL_DATASET}/{GLOBAL_DATAFILE}\"\n",
    "DIMENSION_SCORES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 1 (C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is best run on CSV data where the column names are in the first row. It can also accept files that are in xlsx formats but it will only take data from the first sheet if there are more than one sheet in the excel file.\n",
    "\n",
    "Limitations: It will not check for differences in capitalization of the same word (since all the words will be changed to lower case before the similarity score is calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 2 (C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of datasets with a reference list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compared columns in question must be identical to the ref list, otherwise they will be penalized more harshly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mIssue with column names, are you sure you entered them correctly?\u001b[0m\n",
      "Column name that fails: 'STOCK_CU_NAME'\n",
      "List of all detected column names: ['AREA', 'WATERBODY', 'GAZETTED_NAME', 'LOCAL_NAME_1', 'LOCAL_NAME_2', 'ANALYSIS_YR', 'SPECIES', 'NATURAL_ADULT_SPAWNERS', 'NATURAL_JACK_SPAWNERS', 'NATURAL_SPAWNERS_TOTAL', 'ADULT_BROODSTOCK_REMOVALS', 'JACK_BROODSTOCK_REMOVALS', 'TOTAL_BROODSTOCK_REMOVALS', 'OTHER_REMOVALS', 'TOTAL_RETURN_TO_RIVER', 'ENUMERATION_METHODS', 'ADULT_PRESENCE', 'JACK_PRESENCE', 'START_DTT', 'END_DTT', 'NATURAL_ADULT_FEMALES', 'NATURAL_ADULT_MALES', 'EFFECTIVE_FEMALES', 'WEIGHTED_PCT_SPAWN', 'WATERSHED_CDE', 'WATERBODY_ID', 'POPULATION', 'RUN_TYPE', 'STREAM_ARRIVAL_DT_FROM', 'STREAM_ARRIVAL_DT_TO', 'START_SPAWN_DT_FROM', 'START_SPAWN_DT_TO', 'PEAK_SPAWN_DT_FROM', 'PEAK_SPAWN_DT_TO', 'END_SPAWN_DT_FROM', 'END_SPAWN_DT_TO', 'ACCURACY', 'PRECISION', 'INDEX_YN', 'RELIABILITY', 'ESTIMATE_STAGE', 'ESTIMATE_CLASSIFICATION', 'NO_INSPECTIONS_USED', 'ESTIMATE_METHOD', 'CREATED_DTT', 'UPDATED_DTT', 'ACT_ID', 'POP_ID', 'GFE_ID']\n",
      "{'dimension': 'Consistency', 'score': 0.49651015228426393}\n"
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "    \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "}  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "\n",
    "# Test Consistency Calculations\n",
    "# Using default thresholds and stop words for both metrics\n",
    "consitancy_tests = Consistency(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    c1_column_names=[\"POPULATION\", \"ESTIMATE_CLASSIFICATION\", \"ESTIMATE_METHOD\"],\n",
    "    c2_column_mapping=column_mapping,\n",
    "    # ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\"\n",
    "    return_type='dataset'\n",
    "    # logging_path='metric_output_logs/'\n",
    ")\n",
    "\n",
    "consistancy_score = calculate_dimension_score(\"Consistency\", scores=consitancy_tests.run_metrics(), weights={})\n",
    "DIMENSION_SCORES.append(consistancy_score)\n",
    "print(consistancy_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 1 (A1, Mixed Data Types, Symbols in Numerics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether there are symbols in numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 2 (A2 Outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find outliers that are 1.5 (or any threshold) times away from the inter-quartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:349: DtypeWarning: Columns (15,18,19,27,29,30,31,32,33,34,35,38,39,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(logging_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "When trying to create one line summary for A2, the following error occurred: can only concatenate str (not \"list\") to str\n",
      "{'dimension': 'Accuracy', 'score': 0.7941176470588236}\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy Calculations\n",
    "# Using default threshold, group by, and min score for A2 metric \n",
    "accuracy_tests = Accuracy(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    # selected_columns=[\" Egg Target \", \" Release/ Transfer Target \", \" Coded Wire Tag Target \", \" Fin Clip Target \", \" Thermal Mark Target \", \" Parentage-based Tag Target \", \" PIT Tag Target \"]\n",
    "    selected_columns=[\"AREA\", \"ANALYSIS_YR\", \"NATURAL_ADULT_SPAWNERS\", \"NATURAL_JACK_SPAWNERS\", \"NATURAL_SPAWNERS_TOTAL\", \"ADULT_BROODSTOCK_REMOVALS\", \"JACK_BROODSTOCK_REMOVALS\", \"TOTAL_BROODSTOCK_REMOVALS\", \"OTHER_REMOVALS\", \"TOTAL_RETURN_TO_RIVER\", \"NATURAL_ADULT_MALES\", \"EFFECTIVE_FEMALES\", \"WEIGHTED_PCT_SPAWN\", \"NO_INSPECTIONS_USED\", \"ACT_ID\", \"POP_ID\", \"GFE_ID\"],\n",
    "    return_type='dataset'\n",
    "    # logging_path='metric_output_logs/'\n",
    ")\n",
    "\n",
    "accuracy_score = calculate_dimension_score(\"Accuracy\", scores=accuracy_tests.run_metrics(), weights={})\n",
    "DIMENSION_SCORES.append(accuracy_score)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completeness Type 1 (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is for removing a column that meets the threshold of the percentage of blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 'Completeness', 'score': 0.8482207305966877}\n"
     ]
    }
   ],
   "source": [
    "completeness_tests = Completeness(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    return_type='dataset'\n",
    "    # logging_path='metric_output_logs/'\n",
    ")\n",
    "\n",
    "completeness_score = calculate_dimension_score(\"Completeness\", completeness_tests.run_metrics(), weights={})\n",
    "DIMENSION_SCORES.append(completeness_score)\n",
    "print(completeness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness (U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness Type 1 (U1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [AREA, WATERBODY, GAZETTED_NAME, LOCAL_NAME_1, LOCAL_NAME_2, ANALYSIS_YR, SPECIES, NATURAL_ADULT_SPAWNERS, NATURAL_JACK_SPAWNERS, NATURAL_SPAWNERS_TOTAL, ADULT_BROODSTOCK_REMOVALS, JACK_BROODSTOCK_REMOVALS, TOTAL_BROODSTOCK_REMOVALS, OTHER_REMOVALS, TOTAL_RETURN_TO_RIVER, ENUMERATION_METHODS, ADULT_PRESENCE, JACK_PRESENCE, START_DTT, END_DTT, NATURAL_ADULT_FEMALES, NATURAL_ADULT_MALES, EFFECTIVE_FEMALES, WEIGHTED_PCT_SPAWN, WATERSHED_CDE, WATERBODY_ID, POPULATION, RUN_TYPE, STREAM_ARRIVAL_DT_FROM, STREAM_ARRIVAL_DT_TO, START_SPAWN_DT_FROM, START_SPAWN_DT_TO, PEAK_SPAWN_DT_FROM, PEAK_SPAWN_DT_TO, END_SPAWN_DT_FROM, END_SPAWN_DT_TO, ACCURACY, PRECISION, INDEX_YN, RELIABILITY, ESTIMATE_STAGE, ESTIMATE_CLASSIFICATION, NO_INSPECTIONS_USED, ESTIMATE_METHOD, CREATED_DTT, UPDATED_DTT, ACT_ID, POP_ID, GFE_ID]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 49 columns]\n",
      "\n",
      "Duplication Score: 100.0%\n",
      "When trying to create one line summary for U1, the following error occurred: '>' not supported between instances of 'str' and 'int'\n",
      "{'dimension': 'Uniqueness', 'score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "uniqueness_tests = Uniqueness(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    return_type='dataset'\n",
    "    # logging_path='metric_output_logs/'\n",
    ")\n",
    "\n",
    "uniqueness_score = calculate_dimension_score(\"Uniqueness\", uniqueness_tests.run_metrics(), weights={})\n",
    "DIMENSION_SCORES.append(uniqueness_score)\n",
    "print(uniqueness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Overall Data Quality Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQ grade for this dataset is: C\n"
     ]
    }
   ],
   "source": [
    "# Call grade calculation here\n",
    "print(f'DQ grade for this dataset is: {calculate_DQ_grade(DIMENSION_SCORES)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
