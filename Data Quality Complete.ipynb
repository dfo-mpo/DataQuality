{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run everything in the **Setup** section. \n",
    "    - Make sure to change the working directory to **your** working directory. The code for this is already there.\n",
    "    - Make sure the Excel document for logging the scores also exists in your working directory, and that the file name is correct.\n",
    "\n",
    "2. Determine *if the test needs to be run* by having a good understanding of what each test is doing. \n",
    "    - Please refer to this document [here](https://086gc.sharepoint.com/:x:/r/sites/PacificSalmonTeam/Shared%20Documents/General/02%20-%20PSSI%20Secretariat%20Teams/04%20-%20Strategic%20Salmon%20Data%20Policy%20and%20Analytics/02%20-%20Data%20Governance/00%20-%20Projects/10%20-%20Data%20Quality/Presentation/DQP%20Demo.xlsx?d=wc15abe6743954df980a05f09fe99a560&csf=1&web=1&e=CJeb6h)\n",
    "\n",
    "3. Some requirements for the datasets:\n",
    "    - The data must be on the **first sheet** in the Excel document.\n",
    "    - The **first row** must be the column names. \n",
    "    - The test won't run if the Excel file is open\n",
    "\n",
    "4. After running all the tests, the Excel document for logging the scores can be uploaded to Sharepoint using the function \"Saving the file to sharepoint\". \n",
    "\n",
    "Note: The Output Reports are used for when a data steward is asking about why their dataset gets a certain score. If the metric is not in Output Reports, then running the test itself will generate an output that can be put into a report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run everything in the set up, and double check the working directory so that the data can be read from that same directory.\n",
    "\n",
    "All of these functions are used in the process of calculating data quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onakd\\AppData\\Local\\Temp\\1\\ipykernel_6904\\1945100915.py:4: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Clear memory\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import nbformat\n",
    "import gc\n",
    "\n",
    "# Import dimentions\n",
    "from dimensions.consistancy import Consistency\n",
    "from dimensions.accuracy import Accuracy\n",
    "from dimensions.completeness import Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set to the correct working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to the same place where you saved the test datasets\n",
    "# os.chdir('C:/Users/luos/OneDrive - DFO-MPO/Python') #change directory\n",
    "os.getcwd()  # check where the directory is (and whether the change was successful or not)\n",
    "GLOBAL_USER = \"OnakD\"\n",
    "GLOBAL_DATASET = \"CU Sampling Sites\"\n",
    "GLOBAL_DATAFILE = \"Conservation_Unit_Data_20220902.csv\"\n",
    "DATA_FILE_PATH = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{GLOBAL_DATASET}/{GLOBAL_DATAFILE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 1 (C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is best run on CSV data where the column names are in the first row. It can also accept files that are in xlsx formats but it will only take data from the first sheet if there are more than one sheet in the excel file.\n",
    "\n",
    "Limitations: It will not check for differences in capitalization of the same word (since all the words will be changed to lower case before the similarity score is calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 2 (C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of datasets with a reference list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compared columns in question must be identical to the ref list, otherwise they will be penalized more harshly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:162: DtypeWarning: Columns (27,28,29,30,31,32,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataset_path, encoding=\"utf-8-sig\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDataset is too large for this test, out of memory!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:166: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype=datetime64[ns])\n",
      "  df = pd.read_excel(dataset_path)\n",
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:162: DtypeWarning: Columns (27,28,29,30,31,32,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataset_path, encoding=\"utf-8-sig\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mIssue with column names, are you sure you entered them correctly?\u001b[0m\n",
      "Column name that fails: 'STOCK_CU_NAME'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:162: DtypeWarning: Columns (27,28,29,30,31,32,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataset_path, encoding=\"utf-8-sig\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all detected column names: ['ACT_ID', 'DESCR', 'ANALYSIS_YR', 'STREAM_ID', 'AREA', 'SPECIES', 'SEN_STATUS', 'ESTIMATE_CLASSIFICATION', 'ESTIMATE_METHOD', 'WATERSHED_CDE', 'ESTIMATE_STAGE', 'SPL_ID', 'SEN_PRESENCE_ADULT', 'SEN_PRESENCE_JACK', 'NATURAL_ADULT_SPAWNERS', 'NATURAL_JACK_SPAWNERS', 'NATURAL_SPAWNERS_TOTAL', 'ADULT_BROODSTOCK_REMOVALS', 'JACK_BROODSTOCK_REMOVALS', 'TOTAL_BROODSTOCK_REMOVALS', 'OTHER_REMOVALS', 'TOTAL_RETURN_TO_RIVER', 'UNSPECIFIED_RETURN', 'NO_INSPECTIONS_USED', 'POPULATION', 'MAX_ESTIMATE', 'RUN_TYPE', 'SEN_NUSEDS1_ENUM_METHOD1', 'SEN_NUSEDS1_ENUM_METHOD2', 'SEN_NUSEDS1_ENUM_METHOD3', 'SEN_NUSEDS1_ENUM_METHOD4', 'SEN_NUSEDS1_ENUM_METHOD5', 'SEN_NUSEDS1_ENUM_METHOD6', 'EFFECTIVE_FEMALES', 'WEIGHTED_PCT_SPAWN', 'OTHER_ADULT_REMOVALS', 'OTHER_JACK_REMOVALS', 'TOT_ADULT_RET_RIVER', 'TOT_JACK_RET_RIVER', 'JUV_PRES_TYP', 'GEOGRAPHICAL_EXTNT_OF_ESTIMATE', 'POP_ID', 'CU_NAME', 'CU_INDEX', 'CU_TYPE', 'SPECIES_QUALIFIED', 'SBJ_ID']\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:166: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype=datetime64[ns])\n",
      "  df = pd.read_excel(dataset_path)\n"
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "    \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "}  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "\n",
    "# Test Consistency Calculations\n",
    "# Using default thresholds and stop words for both metrics\n",
    "consitancy_tests = Consistency(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    c1_column_names=[\"DESCR\"],\n",
    "    c2_column_mapping=column_mapping\n",
    "    # ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\"\n",
    ")\n",
    "\n",
    "print(consitancy_tests.run_metrics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 1 (A1, Mixed Data Types, Symbols in Numerics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether there are symbols in numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 2 (A2 Outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some description for this test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 3 (A3 Duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some description for this test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:164: DtypeWarning: Columns (27,28,29,30,31,32,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataset_path, encoding=\"utf-8-sig\")\n",
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:164: DtypeWarning: Columns (27,28,29,30,31,32,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataset_path, encoding=\"utf-8-sig\")\n",
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:164: DtypeWarning: Columns (27,28,29,30,31,32,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataset_path, encoding=\"utf-8-sig\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [ACT_ID, DESCR, ANALYSIS_YR, STREAM_ID, AREA, SPECIES, SEN_STATUS, ESTIMATE_CLASSIFICATION, ESTIMATE_METHOD, WATERSHED_CDE, ESTIMATE_STAGE, SPL_ID, SEN_PRESENCE_ADULT, SEN_PRESENCE_JACK, NATURAL_ADULT_SPAWNERS, NATURAL_JACK_SPAWNERS, NATURAL_SPAWNERS_TOTAL, ADULT_BROODSTOCK_REMOVALS, JACK_BROODSTOCK_REMOVALS, TOTAL_BROODSTOCK_REMOVALS, OTHER_REMOVALS, TOTAL_RETURN_TO_RIVER, UNSPECIFIED_RETURN, NO_INSPECTIONS_USED, POPULATION, MAX_ESTIMATE, RUN_TYPE, SEN_NUSEDS1_ENUM_METHOD1, SEN_NUSEDS1_ENUM_METHOD2, SEN_NUSEDS1_ENUM_METHOD3, SEN_NUSEDS1_ENUM_METHOD4, SEN_NUSEDS1_ENUM_METHOD5, SEN_NUSEDS1_ENUM_METHOD6, EFFECTIVE_FEMALES, WEIGHTED_PCT_SPAWN, OTHER_ADULT_REMOVALS, OTHER_JACK_REMOVALS, TOT_ADULT_RET_RIVER, TOT_JACK_RET_RIVER, JUV_PRES_TYP, GEOGRAPHICAL_EXTNT_OF_ESTIMATE, POP_ID, CU_NAME, CU_INDEX, CU_TYPE, SPECIES_QUALIFIED, SBJ_ID]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 47 columns]\n",
      "\n",
      "Duplication Score: 100.0%\n",
      "[1.0, {'ACT_ID': 0.966184040873678, 'ANALYSIS_YR': 1.0, 'STREAM_ID': 0.9815277469638947, 'SPL_ID': 0.9659943345326633, 'NATURAL_ADULT_SPAWNERS': 0.9815534698575916, 'NATURAL_JACK_SPAWNERS': 0.9930258804464208, 'NATURAL_SPAWNERS_TOTAL': 0.991781535463832}, 1.0, None]\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy Calculations\n",
    "# Using default threshold, group by, and min score for A2 metric \n",
    "accuracy_tests = Accuracy(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    # selected_columns=[\" Egg Target \", \" Release/ Transfer Target \", \" Coded Wire Tag Target \", \" Fin Clip Target \", \" Thermal Mark Target \", \" Parentage-based Tag Target \", \" PIT Tag Target \"]\n",
    "    selected_columns=[\"ACT_ID\",\"ANALYSIS_YR\",\"STREAM_ID\",\"SPL_ID\",\"NATURAL_ADULT_SPAWNERS\",\"NATURAL_JACK_SPAWNERS\",\"NATURAL_SPAWNERS_TOTAL\"]\n",
    ")\n",
    "\n",
    "print(accuracy_tests.run_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is for removing a column that meets the threshold of the percentage of blanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:164: DtypeWarning: Columns (27,28,29,30,31,32,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataset_path, encoding=\"utf-8-sig\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9240346358763629]\n"
     ]
    }
   ],
   "source": [
    "completeness_tests = Completeness(\n",
    "    dataset_path=DATA_FILE_PATH\n",
    ")\n",
    "\n",
    "print(completeness_tests.run_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def calc_timeliness(refresh_date, cycle_day):\n",
    "    refresh_date = pd.to_datetime(refresh_date)\n",
    "    unupdate_cycle = np.max([((datetime.now() - refresh_date).days / cycle_day) - 1, 0])\n",
    "\n",
    "    # unupdate_cycle = np.floor((datetime.now() - refresh_date).days/cycle_day)\n",
    "    # print((datetime.now() - refresh_date).days/cycle_day)\n",
    "    return np.max([0, 100 - (unupdate_cycle * (100 / 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.21004566210046"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_timeliness(\"2022-12-01\", cycle_day=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Reports\n",
    "Run all the functions above first before running this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that output reports can be generated through the data quality tests of\n",
    "<p>    - Consistency type 1\n",
    "<p>    - Accuracy type 2\n",
    "<p>    - Accuracy type 3\n",
    "<p>    - Completeness\n",
    "<p>          \n",
    "<p>  *Completeness test does not require an output report (just find the blanks in the dataset). The rest can be found below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_selected_cells_from_util('utils', 'dq_utils.ipynb', [13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTest failed!\u001b[0m\n",
      "Error: [Errno 2] No such file or directory: 'data/test/Salmonid_Enhancement_Program_Releases.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    column_mapping = {\n",
    "        \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "        \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "    }  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "    compare_datasets(\n",
    "        dataset_path=\"data/test/Salmonid_Enhancement_Program_Releases.xlsx\",\n",
    "        column_mapping=column_mapping,\n",
    "        ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_selected_cells_from_util('utils', 'dq_utils.ipynb', [15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTest failed!\u001b[0m\n",
      "Error: [Errno 2] No such file or directory: 'data/test/SEP Facilities.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    add_only_numbers_columns(\n",
    "        dataset_path=\"data/test/SEP Facilities.xlsx\", selected_columns=[\"LicNo\", \"FRN\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-12-05 13:53:16'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(dataset_path):\n",
    "    # Extract the file name from the path (e.g., 'Dataset_A.csv')\n",
    "    file_name = os.path.basename(dataset_path)\n",
    "    # Split the file name to remove the extension (e.g., 'Dataset_A')\n",
    "    dataset_name = os.path.splitext(file_name)[0]\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More up to date code for the score log can be found in the \"Setup\" section, the code here is treated more as a testing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log a new row into the DQS_Log.xlsx file\n",
    "def log_score(test_name, dataset_name, score, threshold=None):\n",
    "    # Convert score to a percentage\n",
    "    percentage_score = score * 100\n",
    "\n",
    "    # Load the Excel file into a DataFrame\n",
    "    log_file = \"DQS_Log_Test.xlsx\"\n",
    "\n",
    "    # Set threshold to \"No threshold\" if it is not provided\n",
    "    if threshold is None:\n",
    "        threshold_value = \"no threshold\"\n",
    "    else:\n",
    "        threshold_value = threshold\n",
    "    # Try loading the existing Excel file\n",
    "    try:\n",
    "        df = read_data(log_file)\n",
    "    except FileNotFoundError:\n",
    "        # Create an empty DataFrame if file doesn't exist (shouldn't be the case if you already created it)\n",
    "        df = pd.DataFrame(\n",
    "            columns=[\"Dataset\", \"Test\", \"Threshold\", \"Date_Calculated\", \"Score\"]\n",
    "        )\n",
    "\n",
    "    # Prepare the new row as a DataFrame\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Dataset\": [dataset_name],\n",
    "            \"Test\": [test_name],\n",
    "            \"Date_Calculated\": [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"Threshold\": [threshold_value],\n",
    "            \"Score\": [percentage_score],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(log_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the file to sharepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to copy the log file to another folder\n",
    "def copy_log_file(destination_folder):\n",
    "    # Define the name of the file and the current working directory\n",
    "    log_file = \"DQS_Log_Test.xlsx\"\n",
    "\n",
    "    # Get the current working directory (if needed)\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Define the source path (current working directory + file)\n",
    "    source_path = os.path.join(current_directory, log_file)\n",
    "\n",
    "    # Define the destination path (destination folder + file)\n",
    "    destination_path = os.path.join(destination_folder, log_file)\n",
    "\n",
    "    # Copy the file to the destination folder\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print(f\"File copied to {destination_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function when saving the excel document from the working directory to Sharepoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTest failed!\u001b[0m\n",
      "Error: [Errno 2] No such file or directory: 'c:\\\\Users\\\\onakd\\\\Documents\\\\Data Quality Tests\\\\DataQuality\\\\DQS_Log_Test.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Specify the destination folder where you want to copy the file\n",
    "    destination_folder = \"C:/Users/EwertM/Documents/Portal/DataQuality\"\n",
    "\n",
    "    copy_log_file(destination_folder)\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
