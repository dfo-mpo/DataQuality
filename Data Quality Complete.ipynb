{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run everything in the **Setup** section. \n",
    "    - Make sure to change the working directory to **your** working directory. The code for this is already there.\n",
    "    - Make sure the Excel document for logging the scores also exists in your working directory, and that the file name is correct.\n",
    "\n",
    "2. Determine *if the test needs to be run* by having a good understanding of what each test is doing. \n",
    "    - Please refer to this document [here](https://086gc.sharepoint.com/:x:/r/sites/PacificSalmonTeam/Shared%20Documents/General/02%20-%20PSSI%20Secretariat%20Teams/04%20-%20Strategic%20Salmon%20Data%20Policy%20and%20Analytics/02%20-%20Data%20Governance/00%20-%20Projects/10%20-%20Data%20Quality/Presentation/DQP%20Demo.xlsx?d=wc15abe6743954df980a05f09fe99a560&csf=1&web=1&e=CJeb6h)\n",
    "\n",
    "3. Some requirements for the datasets:\n",
    "    - The data must be on the **first sheet** in the Excel document.\n",
    "    - The **first row** must be the column names. \n",
    "    - The test won't run if the Excel file is open\n",
    "\n",
    "4. After running all the tests, the Excel document for logging the scores can be uploaded to Sharepoint using the function \"Saving the file to sharepoint\". \n",
    "\n",
    "Note: The Output Reports are used for when a data steward is asking about why their dataset gets a certain score. If the metric is not in Output Reports, then running the test itself will generate an output that can be put into a report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run everything in the set up, and double check the working directory so that the data can be read from that same directory.\n",
    "\n",
    "All of these functions are used in the process of calculating data quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luos\\AppData\\Local\\Temp\\1\\ipykernel_22536\\1945100915.py:4: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Clear memory\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import nbformat\n",
    "import gc\n",
    "import traceback\n",
    "import nbimporter\n",
    "import utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support functions to allow running cells from other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selected_cells(notebook_path, cell_indices):       \n",
    "    # Load the notebook       \n",
    "    with open(notebook_path) as f:           \n",
    "        nb = nbformat.read(f, as_version=4)   \n",
    "\n",
    "    # Get the cells to run       \n",
    "    selected_cells = [nb.cells[i] for i in cell_indices]   \n",
    "\n",
    "    # Execute the selected cells       \n",
    "    for cell in selected_cells:    \n",
    "        # print(cell)       \n",
    "        if cell.cell_type == 'code':               \n",
    "            exec(cell.source, globals())\n",
    "\n",
    "def run_selected_cells_from_util(util_folder, notebook_name, cell_indices):       \n",
    "    notebook_path = os.path.join(util_folder, notebook_name)       \n",
    "    run_selected_cells(notebook_path, cell_indices)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set to the correct working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to the same place where you saved the test datasets\n",
    "# os.chdir('C:/Users/luos/OneDrive - DFO-MPO/Python') #change directory\n",
    "os.getcwd()  # check where the directory is (and whether the change was successful or not)\n",
    "GLOBAL_USER = \"LuoS\"\n",
    "GLOBAL_DATASET = \"11- NuSEDS All_Areas\" # the folder that stores all the files related to this data\n",
    "GLOBAL_DATASET_PATH = \"ForPortal_20250318\" # the actual folder the data is in\n",
    "GLOBAL_DATAFILE = \"All Areas NuSEDS.csv\" # dataset file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the following functions from dq_utils.ipynb:\n",
    "* read_data(dataset_path) - Function to read either csv or xlsx data\n",
    "* log_score(test_name, dataset_name, score, selected_columns, threshold) - Function to log the scores into an xlsx file (already created, existing)\n",
    "* get_dataset_name(dataset_path) - Function to extract dataset name from a path\n",
    "* Global variables for console output colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_selected_cells_from_util('utils', 'dq_utils.ipynb', [2, 4, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 1 (C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is best run on CSV data where the column names are in the first row. It can also accept files that are in xlsx formats but it will only take data from the first sheet if there are more than one sheet in the excel file.\n",
    "\n",
    "Limitations: It will not check for differences in capitalization of the same word (since all the words will be changed to lower case before the similarity score is calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for C1 \n",
    "run_selected_cells_from_util('utils', 'consistancy_utils.ipynb', [2])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/09 - Pacific Salmon Data Portal/14_Official_Launch/03_Datasets/New Datasets/{GLOBAL_DATASET}/{GLOBAL_DATASET_PATH}/{GLOBAL_DATAFILE}\"\n",
    "    # Test Consistency Calculations\n",
    "\n",
    "    processed_df = process_and_calculate_similarity(\n",
    "        dataset_path=datafilepath,\n",
    "        column_names=[\"project_name\"],\n",
    "        threshold=0.91\n",
    "    )\n",
    "\n",
    "    # processed_df['Overall Consistency Score'].min()\n",
    "    print(processed_df)\n",
    "except MemoryError as e:\n",
    "    print(f'{RED}Dataset is too large for this test, out of memory!{RESET}')\n",
    "    print(f'Error: {e}')\n",
    "except KeyError as e:\n",
    "    print(f'{RED}Issue with column names, are you sure you entered them correctly?{RESET}')\n",
    "    print(f'Column name that fails: {e}')\n",
    "    print(f'List of all detected column names: {list(read_data(datafilepath).columns)}')\n",
    "except FileNotFoundError as e:\n",
    "    print(f'{RED}Did not find dataset, make sure you have provided the correct name.{RESET}')\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 2 (C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of datasets with a reference list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compared columns in question must be identical to the ref list, otherwise they will be penalized more harshly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for C2\n",
    "run_selected_cells_from_util('utils', 'consistancy_utils.ipynb', [5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Get names used for a single column\n",
    "def get_names_used_for_column(df, column_name):\n",
    "    unique_observations = pd.unique(df[column_name].dropna().values.ravel())\n",
    "    return unique_observations\n",
    "\n",
    "\n",
    "# Function 2: Calculate Cosine Similarity\n",
    "def calculate_cosine_similarity(text_list, ref_list, Stop_Words):\n",
    "    count_vectorizer = CountVectorizer(stop_words=Stop_Words)\n",
    "    ref_vec = count_vectorizer.fit_transform(ref_list).todense()\n",
    "    ref_vec_array = np.array(ref_vec)\n",
    "    text_vec = count_vectorizer.transform(text_list).todense()\n",
    "    text_vec_array = np.array(text_vec)\n",
    "    cosine_sim = np.round((cosine_similarity(text_vec_array, ref_vec_array)), 2)\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "# Function 3: Average Consistency Score\n",
    "def average_consistency_score(cosine_sim_df, threshold=0.91):\n",
    "    num_rows, num_columns = cosine_sim_df.shape\n",
    "    total_count = 0  # This will count all values above or equal to the threshold\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        if np.max(cosine_sim_df[i]) >= threshold:  # Include all comparisons\n",
    "            total_count += 1\n",
    "    total_observations = num_rows  # Total number of observations\n",
    "    average_consistency_score = total_count / total_observations\n",
    "    return average_consistency_score\n",
    "\n",
    "\n",
    "def process_and_calculate_similarity_ref(\n",
    "    dataset_path,\n",
    "    column_mapping,\n",
    "    ref_dataset_path=None,\n",
    "    threshold=0.91,\n",
    "    Stop_Words=\"activity\",\n",
    "):\n",
    "    \n",
    "    # Variables that prepare for output reports\n",
    "    errors = None\n",
    "    test_fail_comment = None\n",
    "    all_consistency_scores = None  # Ensure it exists even if errors occur\n",
    "    \n",
    "    try:\n",
    "        # Read the data file\n",
    "        df = read_data(dataset_path)\n",
    "\n",
    "        # Initialize ref_df if a ref dataset is provided\n",
    "        if ref_dataset_path:\n",
    "            df_ref = read_data(ref_dataset_path)\n",
    "            ref_data = True  # Flag to indicate we are using a ref dataset\n",
    "        else:\n",
    "            ref_data = False  # No ref dataset, compare within the same dataset\n",
    "\n",
    "        all_consistency_scores = []\n",
    "\n",
    "        for selected_column, m_selected_column in column_mapping.items():\n",
    "            if ref_data:\n",
    "                # Compare to ref dataset\n",
    "                unique_observations = get_names_used_for_column(df_ref, m_selected_column)\n",
    "            else:\n",
    "                # Use own column for comparison\n",
    "                unique_observations = get_names_used_for_column(df, selected_column)\n",
    "\n",
    "            cosine_sim_matrix = calculate_cosine_similarity(\n",
    "                df[selected_column].dropna(), unique_observations, Stop_Words=Stop_Words\n",
    "            )\n",
    "            column_consistency_score = average_consistency_score(\n",
    "                cosine_sim_matrix, threshold\n",
    "            )\n",
    "            all_consistency_scores.append(column_consistency_score)\n",
    "\n",
    "    except Exception:\n",
    "        # Capture any unexpected error\n",
    "        errors = traceback.format_exc().strip()\n",
    "        test_fail_comment = errors # not sure how to make it say the first part of the error like with the memory issue.\n",
    "    \n",
    "    # Calculate the average of all consistency scores\n",
    "    overall_avg_consistency = (\n",
    "        sum(all_consistency_scores) / len(all_consistency_scores)\n",
    "        if all_consistency_scores\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Consistency (C2)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=column_mapping,\n",
    "        threshold=threshold,\n",
    "        score=overall_avg_consistency,\n",
    "    )\n",
    "\n",
    "    # output report of results\n",
    "    output_log_score(\n",
    "        test_name = \"C2\", \n",
    "        dataset_name = get_dataset_name(dataset_path), \n",
    "        score = overall_avg_consistency, \n",
    "        selected_columns=column_mapping, \n",
    "        new_or_existing = \"existing\", \n",
    "        test_fail_comment = test_fail_comment, \n",
    "        errors = errors, \n",
    "        dimension = \"Consistency\", \n",
    "        threshold= threshold)\n",
    "    \n",
    "\n",
    "    return overall_avg_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:8: DtypeWarning: Columns (15,18,19,30,31,32,33,34,35,38,39,45) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    \"FULL_CU_IN\": \"Full Conservation Unit Index\", }  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "\n",
    "datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/09 - Pacific Salmon Data Portal/14_Official_Launch/03_Datasets/New Datasets/{GLOBAL_DATASET}/{GLOBAL_DATASET_PATH}/{GLOBAL_DATAFILE}\"\n",
    "    # Test Consistency Calculations\n",
    "\n",
    "ref_datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/09 - Pacific Salmon Data Portal/14_Official_Launch/03_Datasets/New Datasets/01 - Pacific Salmon Unit Crosswalk/CrossWalkData_2025-03-14.csv\", \n",
    "\n",
    "c2_score = process_and_calculate_similarity_ref(\n",
    "    dataset_path=datafilepath,\n",
    "    column_mapping=column_mapping,\n",
    "    ref_dataset_path=ref_datafilepath,\n",
    "    threshold=1,\n",
    "    Stop_Words=[\"\"],\n",
    ")\n",
    "\n",
    "print(c2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 1 (A1, Mixed Data Types, Symbols in Numerics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether there are symbols in numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for A1\n",
    "run_selected_cells_from_util('utils', 'accuracy_utils.ipynb', [2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the dataset by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/09 - Pacific Salmon Data Portal/14_Official_Launch/03_Datasets/New Datasets/{GLOBAL_DATASET}/{GLOBAL_DATASET_PATH}/{GLOBAL_DATAFILE}\"\n",
    "    score = accuracy_score(\n",
    "        dataset_path=datafilepath,\n",
    "        selected_columns=[\"project_total_cost\", \"outcome_value\"],\n",
    "    )\n",
    "    print(score)\n",
    "except KeyError as e:\n",
    "    print(f'{RED}Issue with column names, are you sure you entered them correctly?{RESET}')\n",
    "    print(f'Column name that fails: {e}')\n",
    "    print(f'List of all detected column names: {list(read_data(datafilepath).columns)}')\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 2 (A2 Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for A2\n",
    "run_selected_cells_from_util('utils', 'accuracy_utils.ipynb', [4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'project_total_cost': target_species\n",
      "Bull Trout, Chinook Salmon, Coho Salmon, Rainbow Trout                                                                                     1.0\n",
      "Bull Trout, Chinook Salmon, Coho Salmon, Rainbow Trout, Steelhead Trout                                                                    1.0\n",
      "Bull Trout, Chinook Salmon, Coho Salmon, Sockeye Salmon, Steelhead Trout                                                                   1.0\n",
      "Bull Trout, Chinook Salmon, Coho Salmon, Steelhead Trout                                                                                   1.0\n",
      "Bull Trout, Steelhead Trout, Chinook Salmon, Coho Salmon, Sockeye Salmon, White Sturgeon, Mountain Sucker, Rocky Mountain Ridged Mussel    1.0\n",
      "                                                                                                                                          ... \n",
      "Umatilla Dace, White Sturgeon, Columbia Sculpin, Shorthead Sculpin                                                                         1.0\n",
      "Westslope Cutthroat Trout                                                                                                                  1.0\n",
      "Westslope Cutthroat Trout, Chinook Salmon, SO                                                                                              1.0\n",
      "Westslope Cutthroat Trout, Chinook Salmon, Sockeye Salmon, Coho Salmon                                                                     1.0\n",
      "White Sturgeon, Umatilla Dace, Westslope Cutthroat Trout, Columbia Sculpin, Shorthead Sculpin                                              1.0\n",
      "Name: project_total_cost, Length: 123, dtype: float64}, {'project_total_cost': 0.943089430894309})\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/09 - Pacific Salmon Data Portal/14_Official_Launch/03_Datasets/New Datasets/{GLOBAL_DATASET}/{GLOBAL_DATASET_PATH}/{GLOBAL_DATAFILE}\"\n",
    "    outliers = find_outliers_iqr(\n",
    "        dataset_path=datafilepath,\n",
    "        selected_columns = [\"project_total_cost\"],\n",
    "        groupby_column = [\"target_species\"], \n",
    "        threshold=0.90,\n",
    "        minimum_score=0.85,\n",
    "    )\n",
    "    print(outliers)\n",
    "except KeyError as e:\n",
    "    print(f'{RED}Issue with column names, are you sure you entered them correctly?{RESET}')\n",
    "    print(f'Column name that fails: {e}')\n",
    "    print(f'List of all detected column names: {list(read_data(datafilepath).columns)}')\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness Type 1 (U1, which was A3 Duplicates before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run utils for A3\n",
    "run_selected_cells_from_util('utils', 'accuracy_utils.ipynb', [6]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "                     activity_id  \\\n",
      "31   GC-2016-00017_FY18-19_S1_A1   \n",
      "32   GC-2016-00017_FY18-19_S1_A1   \n",
      "33   GC-2016-00017_FY18-19_S2_A1   \n",
      "34   GC-2016-00017_FY18-19_S2_A1   \n",
      "775  GC-2016-00017_FY18-19_S1_A2   \n",
      "776  GC-2016-00017_FY18-19_S1_A2   \n",
      "777  GC-2016-00017_FY18-19_S2_A2   \n",
      "778  GC-2016-00017_FY18-19_S2_A2   \n",
      "\n",
      "                                          project_name  \\\n",
      "31   Activity 1: Elaho River Rock Obstruction Resto...   \n",
      "32   Activity 1: Elaho River Rock Obstruction Resto...   \n",
      "33   Activity 1: Elaho River Rock Obstruction Resto...   \n",
      "34   Activity 1: Elaho River Rock Obstruction Resto...   \n",
      "775  Activity 1: Elaho River Rock Obstruction Resto...   \n",
      "776  Activity 1: Elaho River Rock Obstruction Resto...   \n",
      "777  Activity 1: Elaho River Rock Obstruction Resto...   \n",
      "778  Activity 1: Elaho River Rock Obstruction Resto...   \n",
      "\n",
      "                                   project_description  \\\n",
      "31   The Elaho River watershed is a tributary of Sq...   \n",
      "32   The Elaho River watershed is a tributary of Sq...   \n",
      "33   The Elaho River watershed is a tributary of Sq...   \n",
      "34   The Elaho River watershed is a tributary of Sq...   \n",
      "775  The Elaho River watershed is a tributary of Sq...   \n",
      "776  The Elaho River watershed is a tributary of Sq...   \n",
      "777  The Elaho River watershed is a tributary of Sq...   \n",
      "778  The Elaho River watershed is a tributary of Sq...   \n",
      "\n",
      "                          project_lead  \\\n",
      "31   Squamish River Watershed Society    \n",
      "32   Squamish River Watershed Society    \n",
      "33   Squamish River Watershed Society    \n",
      "34   Squamish River Watershed Society    \n",
      "775  Squamish River Watershed Society    \n",
      "776  Squamish River Watershed Society    \n",
      "777  Squamish River Watershed Society    \n",
      "778  Squamish River Watershed Society    \n",
      "\n",
      "                                      project_partners  project_total_cost  \\\n",
      "31   Squamish Nation, Province of British Columbia ...           111666.65   \n",
      "32   Squamish Nation, Province of British Columbia ...           111666.65   \n",
      "33   Squamish Nation, Province of British Columbia ...           111666.65   \n",
      "34   Squamish Nation, Province of British Columbia ...           111666.65   \n",
      "775  Squamish Nation, Province of British Columbia ...           111666.65   \n",
      "776  Squamish Nation, Province of British Columbia ...           111666.65   \n",
      "777  Squamish Nation, Province of British Columbia ...           111666.65   \n",
      "778  Squamish Nation, Province of British Columbia ...           111666.65   \n",
      "\n",
      "    project_duration reporting_fiscal_year  site_latitude  site_longitude  \\\n",
      "31         2016-2019             2018-2019      50.121359     -123.445288   \n",
      "32         2016-2019             2018-2019      50.121359     -123.445288   \n",
      "33         2016-2019             2018-2019      50.130101     -123.472582   \n",
      "34         2016-2019             2018-2019      50.130101     -123.472582   \n",
      "775        2016-2019             2018-2019      50.121359     -123.445288   \n",
      "776        2016-2019             2018-2019      50.121359     -123.445288   \n",
      "777        2016-2019             2018-2019      50.130101     -123.472582   \n",
      "778        2016-2019             2018-2019      50.130101     -123.472582   \n",
      "\n",
      "    ecosystem_types               target_species other_benefitting_species  \\\n",
      "31       Freshwater  Chinook Salmon, Coho Salmon                       NaN   \n",
      "32       Freshwater  Chinook Salmon, Coho Salmon                       NaN   \n",
      "33       Freshwater  Chinook Salmon, Coho Salmon                       NaN   \n",
      "34       Freshwater  Chinook Salmon, Coho Salmon                       NaN   \n",
      "775      Freshwater  Chinook Salmon, Coho Salmon                       NaN   \n",
      "776      Freshwater  Chinook Salmon, Coho Salmon                       NaN   \n",
      "777      Freshwater  Chinook Salmon, Coho Salmon                       NaN   \n",
      "778      Freshwater  Chinook Salmon, Coho Salmon                       NaN   \n",
      "\n",
      "    restoration_activity restoration_outcome  outcome_value  \n",
      "31          Fish passage                 NaN            NaN  \n",
      "32          Fish passage                 NaN            NaN  \n",
      "33          Fish passage                 NaN            NaN  \n",
      "34          Fish passage                 NaN            NaN  \n",
      "775                  NaN                 NaN            NaN  \n",
      "776                  NaN                 NaN            NaN  \n",
      "777                  NaN                 NaN            NaN  \n",
      "778                  NaN                 NaN            NaN  \n",
      "\n",
      "Duplication Score: 99.46236559139786%\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/09 - Pacific Salmon Data Portal/14_Official_Launch/03_Datasets/New Datasets/{GLOBAL_DATASET}/{GLOBAL_DATASET_PATH}/{GLOBAL_DATAFILE}\"\n",
    "    find_duplicates_and_percentage(\n",
    "        dataset_path=datafilepath\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is for removing a column that meets the threshold of the percentage of blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_selected_cells_from_util('utils', 'dq_utils.ipynb', [10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datafilepath = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/09 - Pacific Salmon Data Portal/14_Official_Launch/03_Datasets/New Datasets/{GLOBAL_DATASET}/{GLOBAL_DATASET_PATH}/{GLOBAL_DATAFILE}\"\n",
    "    completeness_test(\n",
    "        dataset_path = datafilepath,\n",
    "        exclude_columns=[\"project_partners\", \"project_total_cost\", \"site_latitude\", \"site_longitude\", \"ecosystem_types\", \"target_species\", \"other_benefitting_species\", \"restoration_activity\", \"restoration_outcome\", \"outcome_value\"], \n",
    "        threshold=0.75,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def calc_timeliness(refresh_date, cycle_day):\n",
    "    refresh_date = pd.to_datetime(refresh_date)\n",
    "    unupdate_cycle = np.max([((datetime.now() - refresh_date).days / cycle_day) - 1, 0])\n",
    "\n",
    "    # unupdate_cycle = np.floor((datetime.now() - refresh_date).days/cycle_day)\n",
    "    # print((datetime.now() - refresh_date).days/cycle_day)\n",
    "    return np.max([0, 100 - (unupdate_cycle * (100 / 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.21004566210046"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_timeliness(\"2022-12-01\", cycle_day=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Reports\n",
    "Run all the functions above first before running this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that output reports can be generated through the data quality tests of\n",
    "<p>    - Consistency type 1\n",
    "<p>    - Accuracy type 2\n",
    "<p>    - Accuracy type 3\n",
    "<p>    - Completeness\n",
    "<p>          \n",
    "<p>  *Completeness test does not require an output report (just find the blanks in the dataset). The rest can be found below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_selected_cells_from_util('utils', 'dq_utils.ipynb', [13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTest failed!\u001b[0m\n",
      "Error: [Errno 2] No such file or directory: 'data/test/Salmonid_Enhancement_Program_Releases.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    column_mapping = {\n",
    "        \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "        \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "    }  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "    compare_datasets(\n",
    "        dataset_path=\"data/test/Salmonid_Enhancement_Program_Releases.xlsx\",\n",
    "        column_mapping=column_mapping,\n",
    "        ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_selected_cells_from_util('utils', 'dq_utils.ipynb', [15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTest failed!\u001b[0m\n",
      "Error: [Errno 2] No such file or directory: 'data/test/SEP Facilities.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    add_only_numbers_columns(\n",
    "        dataset_path=\"data/test/SEP Facilities.xlsx\", selected_columns=[\"LicNo\", \"FRN\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-12-05 13:53:16'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(dataset_path):\n",
    "    # Extract the file name from the path (e.g., 'Dataset_A.csv')\n",
    "    file_name = os.path.basename(dataset_path)\n",
    "    # Split the file name to remove the extension (e.g., 'Dataset_A')\n",
    "    dataset_name = os.path.splitext(file_name)[0]\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More up to date code for the score log can be found in the \"Setup\" section, the code here is treated more as a testing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log a new row into the DQS_Log.xlsx file\n",
    "def log_score(test_name, dataset_name, score, threshold=None):\n",
    "    # Convert score to a percentage\n",
    "    percentage_score = score * 100\n",
    "\n",
    "    # Load the Excel file into a DataFrame\n",
    "    log_file = \"DQS_Log_Test.xlsx\"\n",
    "\n",
    "    # Set threshold to \"No threshold\" if it is not provided\n",
    "    if threshold is None:\n",
    "        threshold_value = \"no threshold\"\n",
    "    else:\n",
    "        threshold_value = threshold\n",
    "    # Try loading the existing Excel file\n",
    "    try:\n",
    "        df = read_data(log_file)\n",
    "    except FileNotFoundError:\n",
    "        # Create an empty DataFrame if file doesn't exist (shouldn't be the case if you already created it)\n",
    "        df = pd.DataFrame(\n",
    "            columns=[\"Dataset\", \"Test\", \"Threshold\", \"Date_Calculated\", \"Score\"]\n",
    "        )\n",
    "\n",
    "    # Prepare the new row as a DataFrame\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Dataset\": [dataset_name],\n",
    "            \"Test\": [test_name],\n",
    "            \"Date_Calculated\": [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"Threshold\": [threshold_value],\n",
    "            \"Score\": [percentage_score],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(log_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the file to sharepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to copy the log file to another folder\n",
    "def copy_log_file(destination_folder):\n",
    "    # Define the name of the file and the current working directory\n",
    "    log_file = \"DQS_Log_Test.xlsx\"\n",
    "\n",
    "    # Get the current working directory (if needed)\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Define the source path (current working directory + file)\n",
    "    source_path = os.path.join(current_directory, log_file)\n",
    "\n",
    "    # Define the destination path (destination folder + file)\n",
    "    destination_path = os.path.join(destination_folder, log_file)\n",
    "\n",
    "    # Copy the file to the destination folder\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print(f\"File copied to {destination_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function when saving the excel document from the working directory to Sharepoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTest failed!\u001b[0m\n",
      "Error: [Errno 2] No such file or directory: 'c:\\\\Users\\\\onakd\\\\Documents\\\\Data Quality Tests\\\\DataQuality\\\\DQS_Log_Test.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Specify the destination folder where you want to copy the file\n",
    "    destination_folder = \"C:/Users/EwertM/Documents/Portal/DataQuality\"\n",
    "\n",
    "    copy_log_file(destination_folder)\n",
    "except Exception as e:\n",
    "    print(f'{RED}Test failed!{RESET}')\n",
    "    print(f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
