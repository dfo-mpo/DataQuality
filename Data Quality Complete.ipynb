{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run everything in the **Setup** section. \n",
    "    - Make sure to change the working directory to **your** working directory. The code for this is already there.\n",
    "    - Make sure the Excel document for logging the scores also exists in your working directory, and that the file name is correct.\n",
    "\n",
    "2. Determine *if the test needs to be run* by having a good understanding of what each test is doing. \n",
    "    - Please refer to this document [here](https://086gc.sharepoint.com/:x:/r/sites/PacificSalmonTeam/Shared%20Documents/General/02%20-%20PSSI%20Secretariat%20Teams/04%20-%20Strategic%20Salmon%20Data%20Policy%20and%20Analytics/02%20-%20Data%20Governance/00%20-%20Projects/10%20-%20Data%20Quality/Presentation/DQP%20Demo.xlsx?d=wc15abe6743954df980a05f09fe99a560&csf=1&web=1&e=CJeb6h)\n",
    "\n",
    "3. Some requirements for the datasets:\n",
    "    - The data must be on the **first sheet** in the Excel document.\n",
    "    - The **first row** must be the column names. \n",
    "    - The test won't run if the Excel file is open\n",
    "\n",
    "4. After running all the tests, the Excel document for logging the scores can be uploaded to Sharepoint using the function \"Saving the file to sharepoint\". \n",
    "\n",
    "Note: The Output Reports are used for when a data steward is asking about why their dataset gets a certain score. If the metric is not in Output Reports, then running the test itself will generate an output that can be put into a report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run everything in the set up, and double check the working directory so that the data can be read from that same directory.\n",
    "\n",
    "All of these functions are used in the process of calculating data quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onakd\\AppData\\Local\\Temp\\1\\ipykernel_25308\\1945100915.py:4: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Clear memory\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import nbformat\n",
    "import gc\n",
    "\n",
    "# Import dimentions\n",
    "from dimensions.consistency import Consistency\n",
    "from dimensions.accuracy import Accuracy\n",
    "from dimensions.completeness import Completeness\n",
    "from dimensions.uniqueness import Uniqueness\n",
    "from dimensions.utils import calculate_dimension_score, calculate_DQ_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set to the correct working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to the same place where you saved the test datasets\n",
    "# os.chdir('C:/Users/luos/OneDrive - DFO-MPO/Python') #change directory\n",
    "os.getcwd()  # check where the directory is (and whether the change was successful or not)\n",
    "LOGGING_PATH = \"/metric_output_logs/\"\n",
    "GLOBAL_USER = \"OnakD\"\n",
    "GLOBAL_DATASET = \"NuSEDS Escapement\"\n",
    "GLOBAL_DATAFILE = \"Johnstone Strait and Strait of Georgia NuSEDS_20241004.xlsx\"\n",
    "DATA_FILE_PATH = f\"C:/Users/{GLOBAL_USER}/OneDrive - DFO-MPO/04 - Strategic Salmon Data Policy and Analytics/07 - Data Products & Data/21 - Transitory Files/{GLOBAL_DATASET}/{GLOBAL_DATAFILE}\"\n",
    "DIMENSION_SCORES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 1 (C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is best run on CSV data where the column names are in the first row. It can also accept files that are in xlsx formats but it will only take data from the first sheet if there are more than one sheet in the excel file.\n",
    "\n",
    "Limitations: It will not check for differences in capitalization of the same word (since all the words will be changed to lower case before the similarity score is calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Type 2 (C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate consistency score of datasets with a reference list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compared columns in question must be identical to the ref list, otherwise they will be penalized more harshly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Test Consistency Calculations\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Using default thresholds and stop words for both metrics\u001b[39;00m\n\u001b[0;32m      8\u001b[0m consitancy_tests \u001b[38;5;241m=\u001b[39m Consistency(\n\u001b[0;32m      9\u001b[0m     dataset_path\u001b[38;5;241m=\u001b[39mDATA_FILE_PATH,\n\u001b[0;32m     10\u001b[0m     c1_column_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOPULATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mESTIMATE_CLASSIFICATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mESTIMATE_METHOD\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     logging_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_output_logs/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconsitancy_tests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\consistency.py:227\u001b[0m, in \u001b[0;36mConsistency.run_metrics\u001b[1;34m(self, metrics)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 227\u001b[0m         overall_consistency_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c1_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    229\u001b[0m         overall_consistency_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c2_metric(metric\u001b[38;5;241m.\u001b[39mlower())     \n",
      "File \u001b[1;32mc:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\consistency.py:49\u001b[0m, in \u001b[0;36mConsistency._c1_metric\u001b[1;34m(self, metric)\u001b[0m\n\u001b[0;32m     46\u001b[0m np\u001b[38;5;241m.\u001b[39mfill_diagonal(text_sim_matrix, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Combine text similarity with numeric similarity to get a final similarity matrix\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m combined_sim_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_combined_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_sim_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Output the results of combined_sim_matrix into a dataframe with column names, and the next most similar column names\u001b[39;00m\n\u001b[0;32m     52\u001b[0m max_values_df \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_max_similarity_values(combined_sim_matrix, unique_observations, column_name)\n",
      "File \u001b[1;32mc:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:133\u001b[0m, in \u001b[0;36mcalculate_combined_similarity\u001b[1;34m(unique_observations, text_similarity_matrix)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, obs_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_observations):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;66;03m# Calculate the string similarity for the current pair\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m         seq_sim \u001b[38;5;241m=\u001b[39m \u001b[43mstring_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;66;03m# Update the combined similarity matrix with the maximum value between existing and sequence matcher \u001b[39;00m\n\u001b[0;32m    136\u001b[0m         combined_sim_matrix[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(combined_sim_matrix[i, j], seq_sim)\n",
      "File \u001b[1;32mc:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:69\u001b[0m, in \u001b[0;36mstring_similarity\u001b[1;34m(str1, str2)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstring_similarity\u001b[39m(str1, str2):  \n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSequenceMatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\difflib.py:619\u001b[0m, in \u001b[0;36mSequenceMatcher.ratio\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mratio\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a measure of the sequences' similarity (float in [0,1]).\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03m    Where T is the total number of elements in both sequences, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;124;03m    1.0\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(triple[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m triple \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matching_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _calculate_ratio(matches, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb))\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\difflib.py:454\u001b[0m, in \u001b[0;36mSequenceMatcher.get_matching_blocks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m queue:\n\u001b[0;32m    453\u001b[0m     alo, ahi, blo, bhi \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m--> 454\u001b[0m     i, j, k \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_longest_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43malo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mahi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbhi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# a[alo:i] vs b[blo:j] unknown\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# a[i:i+k] same as b[j:j+k]\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;66;03m# a[i+k:ahi] vs b[j+k:bhi] unknown\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k:   \u001b[38;5;66;03m# if k is 0, there was no matching block\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\difflib.py:386\u001b[0m, in \u001b[0;36mSequenceMatcher.find_longest_match\u001b[1;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     k \u001b[38;5;241m=\u001b[39m newj2len[j] \u001b[38;5;241m=\u001b[39m j2lenget(j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbestsize\u001b[49m:\n\u001b[0;32m    387\u001b[0m         besti, bestj, bestsize \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m-\u001b[39mk\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, j\u001b[38;5;241m-\u001b[39mk\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, k\n\u001b[0;32m    388\u001b[0m j2len \u001b[38;5;241m=\u001b[39m newj2len\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    \"STOCK_CU_NAME\": \"CU_Display\",\n",
    "    \"STOCK_CU_INDEX\": \"FULL_CU_IN\",\n",
    "}  # the pattern for comparison is 'dataset column' : 'reference column'\n",
    "\n",
    "# Test Consistency Calculations\n",
    "# Using default thresholds and stop words for both metrics\n",
    "consitancy_tests = Consistency(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    c1_column_names=[\"POPULATION\", \"ESTIMATE_CLASSIFICATION\", \"ESTIMATE_METHOD\"],\n",
    "    c2_column_mapping=column_mapping,\n",
    "    # ref_dataset_path=\"data/Pacific Salmon Population Unit Crosswalk_Final_20240513.xlsx\"\n",
    "    return_type='dataset',\n",
    "    logging_path='metric_output_logs/'\n",
    ")\n",
    "\n",
    "consistancy_score = calculate_dimension_score(consitancy_tests.run_metrics())\n",
    "DIMENSION_SCORES.append(\"Consistency\", consistancy_score)\n",
    "print(consistancy_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 1 (A1, Mixed Data Types, Symbols in Numerics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether there are symbols in numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Type 2 (A2 Outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find outliers that are 1.5 (or any threshold) times away from the inter-quartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onakd\\Documents\\Data Quality Tests\\DataQuality\\dimensions\\utils.py:327: DtypeWarning: Columns (15,18,19,27,29,30,31,32,33,34,35,38,39,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(logging_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AREA 1.0\n",
      "ANALYSIS_YR 1.0\n",
      "NATURAL_ADULT_SPAWNERS 0.8479542302357836\n",
      "NATURAL_JACK_SPAWNERS 0.8054697838553154\n",
      "NATURAL_SPAWNERS_TOTAL 0.8446242774566474\n",
      "ADULT_BROODSTOCK_REMOVALS 0.8213740458015267\n",
      "JACK_BROODSTOCK_REMOVALS 0.7898433279308482\n",
      "TOTAL_BROODSTOCK_REMOVALS 0.8280044101433297\n",
      "OTHER_REMOVALS 0.8429203539823009\n",
      "TOTAL_RETURN_TO_RIVER 0.8549577747593456\n",
      "NATURAL_ADULT_MALES 1.0\n",
      "EFFECTIVE_FEMALES 1.0\n",
      "WEIGHTED_PCT_SPAWN 1.0\n",
      "NO_INSPECTIONS_USED 0.9796641791044776\n",
      "ACT_ID 0.9433163557864203\n",
      "POP_ID 0.9230276250548116\n",
      "GFE_ID 0.9702330758592775\n",
      "test4\n",
      "1.0\n",
      "['AREA', 'ANALYSIS_YR', 'NATURAL_ADULT_SPAWNERS', 'NATURAL_JACK_SPAWNERS', 'NATURAL_SPAWNERS_TOTAL', 'ADULT_BROODSTOCK_REMOVALS', 'JACK_BROODSTOCK_REMOVALS', 'TOTAL_BROODSTOCK_REMOVALS', 'OTHER_REMOVALS', 'TOTAL_RETURN_TO_RIVER', 'NATURAL_ADULT_MALES', 'EFFECTIVE_FEMALES', 'WEIGHTED_PCT_SPAWN', 'NO_INSPECTIONS_USED', 'ACT_ID', 'POP_ID', 'GFE_ID']\n",
      "test5\n",
      "Coloumns with outliers: ['AREA', 'ANALYSIS_YR', 'TOTAL_RETURN_TO_RIVER', 'NATURAL_ADULT_MALES', 'EFFECTIVE_FEMALES', 'WEIGHTED_PCT_SPAWN', 'NO_INSPECTIONS_USED', 'ACT_ID', 'POP_ID', 'GFE_ID']\n",
      "[1.0, {'AREA': True, 'ANALYSIS_YR': True, 'NATURAL_ADULT_SPAWNERS': False, 'NATURAL_JACK_SPAWNERS': False, 'NATURAL_SPAWNERS_TOTAL': False, 'ADULT_BROODSTOCK_REMOVALS': False, 'JACK_BROODSTOCK_REMOVALS': False, 'TOTAL_BROODSTOCK_REMOVALS': False, 'OTHER_REMOVALS': False, 'TOTAL_RETURN_TO_RIVER': True, 'NATURAL_ADULT_MALES': True, 'EFFECTIVE_FEMALES': True, 'WEIGHTED_PCT_SPAWN': True, 'NO_INSPECTIONS_USED': True, 'ACT_ID': True, 'POP_ID': True, 'GFE_ID': True}]\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy Calculations\n",
    "# Using default threshold, group by, and min score for A2 metric \n",
    "accuracy_tests = Accuracy(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    # selected_columns=[\" Egg Target \", \" Release/ Transfer Target \", \" Coded Wire Tag Target \", \" Fin Clip Target \", \" Thermal Mark Target \", \" Parentage-based Tag Target \", \" PIT Tag Target \"]\n",
    "    selected_columns=[\"AREA\", \"ANALYSIS_YR\", \"NATURAL_ADULT_SPAWNERS\", \"NATURAL_JACK_SPAWNERS\", \"NATURAL_SPAWNERS_TOTAL\", \"ADULT_BROODSTOCK_REMOVALS\", \"JACK_BROODSTOCK_REMOVALS\", \"TOTAL_BROODSTOCK_REMOVALS\", \"OTHER_REMOVALS\", \"TOTAL_RETURN_TO_RIVER\", \"NATURAL_ADULT_MALES\", \"EFFECTIVE_FEMALES\", \"WEIGHTED_PCT_SPAWN\", \"NO_INSPECTIONS_USED\", \"ACT_ID\", \"POP_ID\", \"GFE_ID\"],\n",
    "    return_type='dataset',\n",
    "    logging_path='metric_output_logs/'\n",
    ")\n",
    "\n",
    "accuracy_score = calculate_dimension_score(accuracy_tests.run_metrics())\n",
    "DIMENSION_SCORES.append(\"Accuracy\", accuracy_score)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completeness Type 1 (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is for removing a column that meets the threshold of the percentage of blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8482207305966877]\n"
     ]
    }
   ],
   "source": [
    "completeness_tests = Completeness(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    return_type='dataset',\n",
    "    logging_path='metric_output_logs/'\n",
    ")\n",
    "\n",
    "completeness_score = calculate_dimension_score(completeness_tests.run_metrics())\n",
    "DIMENSION_SCORES.append(\"Completeness\", completeness_score)\n",
    "print(completeness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness (U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness Type 1 (U1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [AREA, WATERBODY, GAZETTED_NAME, LOCAL_NAME_1, LOCAL_NAME_2, ANALYSIS_YR, SPECIES, NATURAL_ADULT_SPAWNERS, NATURAL_JACK_SPAWNERS, NATURAL_SPAWNERS_TOTAL, ADULT_BROODSTOCK_REMOVALS, JACK_BROODSTOCK_REMOVALS, TOTAL_BROODSTOCK_REMOVALS, OTHER_REMOVALS, TOTAL_RETURN_TO_RIVER, ENUMERATION_METHODS, ADULT_PRESENCE, JACK_PRESENCE, START_DTT, END_DTT, NATURAL_ADULT_FEMALES, NATURAL_ADULT_MALES, EFFECTIVE_FEMALES, WEIGHTED_PCT_SPAWN, WATERSHED_CDE, WATERBODY_ID, POPULATION, RUN_TYPE, STREAM_ARRIVAL_DT_FROM, STREAM_ARRIVAL_DT_TO, START_SPAWN_DT_FROM, START_SPAWN_DT_TO, PEAK_SPAWN_DT_FROM, PEAK_SPAWN_DT_TO, END_SPAWN_DT_FROM, END_SPAWN_DT_TO, ACCURACY, PRECISION, INDEX_YN, RELIABILITY, ESTIMATE_STAGE, ESTIMATE_CLASSIFICATION, NO_INSPECTIONS_USED, ESTIMATE_METHOD, CREATED_DTT, UPDATED_DTT, ACT_ID, POP_ID, GFE_ID]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 49 columns]\n",
      "\n",
      "Duplication Score: 100.0%\n",
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "uniqueness_tests = Uniqueness(\n",
    "    dataset_path=DATA_FILE_PATH,\n",
    "    return_type='dataset',\n",
    "    logging_path='metric_output_logs/'\n",
    ")\n",
    "\n",
    "uniqueness_score = calculate_dimension_score(uniqueness_tests.run_metrics())\n",
    "DIMENSION_SCORES.append(\"Uniqueness\", uniqueness_score)\n",
    "print(uniqueness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Overall Data Quality Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call grade calculation here\n",
    "print(f'DQ grade for this dataset is: {calculate_DQ_grade(DIMENSION_SCORES)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
