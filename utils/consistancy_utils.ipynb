{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1 Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is best run on CSV data where the column names are in the first row. It can also accept files that are in xlsx formats but it will only take data from the first sheet if there are more than one sheet in the excel file.\n",
    "\n",
    "Limitations: It will not check for differences in capitalization of the same word (since all the words will be changed to lower case before the similarity score is calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency Type 1 (C1) function\n",
    "\n",
    "# Dictionary mapping Canadian province abbreviations to their full names\n",
    "province_abbreviations = {\n",
    "    \"BC\": \"British Columbia\",\n",
    "    \"ON\": \"Ontario\",\n",
    "    \"QC\": \"Quebec\",\n",
    "    \"AB\": \"Alberta\",\n",
    "    \"MB\": \"Manitoba\",\n",
    "    \"SK\": \"Saskatchewan\",\n",
    "    \"NS\": \"Nova Scotia\",\n",
    "    \"NB\": \"New Brunswick\",\n",
    "    \"NL\": \"Newfoundland and Labrador\",\n",
    "    \"PE\": \"Prince Edward Island\",\n",
    "    \"NT\": \"Northwest Territories\",\n",
    "    \"YT\": \"Yukon\",\n",
    "    \"NU\": \"Nunavut\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_text(text, remove_numbers=False):\n",
    "    \"\"\"\n",
    "    Normalize input text by converting to lowercase, stripping whitespace,\n",
    "    replacing province abbreviations with full names, and removing non-alphanumeric characters.\n",
    "    Optionally remove numbers based on the flag.\n",
    "    \"\"\"\n",
    "    text = str(text).lower().strip()\n",
    "    for abbr, full in province_abbreviations.items():\n",
    "        text = re.sub(r\"\\b\" + abbr.lower() + r\"\\b\", full.lower(), text)\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = \"\".join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def extract_numbers(text):\n",
    "    \"\"\"\n",
    "    Extract all numbers from the input text and return them as a list of strings.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\d+\", text)\n",
    "\n",
    "\n",
    "def remove_short_numbers(text):\n",
    "    \"\"\"\n",
    "    Remove numbers with 1 or 2 digits from the input text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\b\\d{1,4}\\b\", \"\", text)\n",
    "\n",
    "\n",
    "def numeric_similarity(num1_list, num2_list):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two lists of numbers by comparing each digit.\n",
    "    Return the proportion of matching digits.\n",
    "    \"\"\"\n",
    "    num1, num2 = \" \".join(num1_list), \" \".join(num2_list)\n",
    "    matches = sum(1 for a, b in zip(num1, num2) if a == b)\n",
    "    max_length = max(len(num1), len(num2))\n",
    "    return matches / max_length if max_length > 0 else 0\n",
    "\n",
    "\n",
    "def string_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two strings using the SequenceMatcher from difflib.\n",
    "    Return the similarity ratio.\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text_list, ref_list, Stop_Words):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between lists of texts using TF-IDF vectorization.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=Stop_Words, analyzer=\"word\", ngram_range=(1, 2)\n",
    "    )\n",
    "    ref_vec = vectorizer.fit_transform(ref_list)\n",
    "    text_vec = vectorizer.transform(text_list)\n",
    "    return cosine_similarity(text_vec, ref_vec)\n",
    "\n",
    "\n",
    "def contains_short_number(num_list):\n",
    "    \"\"\"\n",
    "    Check if any number in the list has 1 or 2 digits.\n",
    "    \"\"\"\n",
    "    return any(len(num) <= 4 for num in num_list)\n",
    "\n",
    "\n",
    "def numbers_match(num_list1, num_list2):\n",
    "    \"\"\"\n",
    "    Check if any number in the first list is present in the second list.\n",
    "    \"\"\"\n",
    "    return any(num in num_list2 for num in num_list1)\n",
    "\n",
    "\n",
    "def calculate_combined_similarity(df, unique_observations, text_similarity_matrix):\n",
    "    \"\"\"\n",
    "    Combine text and numeric similarities into a single similarity matrix.\n",
    "    \"\"\"\n",
    "    # Make a copy of the text similarity matrix to modify it\n",
    "    combined_sim_matrix = np.copy(text_similarity_matrix)\n",
    "\n",
    "    # Extract numeric parts from each unique observation\n",
    "    numeric_parts = [extract_numbers(obs) for obs in unique_observations]\n",
    "\n",
    "    # Iterate over each pair of unique observations to calculate numeric similarity\n",
    "    for i, num_i in enumerate(numeric_parts):\n",
    "        for j, num_j in enumerate(numeric_parts):\n",
    "            if i != j:\n",
    "                # Calculate the numeric similarity for the current pair\n",
    "                num_sim = numeric_similarity(num_i, num_j)\n",
    "\n",
    "                # Update the combined similarity matrix with the maximum value between text and numeric similarity\n",
    "                combined_sim_matrix[i, j] = max(combined_sim_matrix[i, j], num_sim)\n",
    "\n",
    "    # Iterate over each pair of unique observations to calculate string similarity\n",
    "    for i, obs_i in enumerate(unique_observations):\n",
    "        for j, obs_j in enumerate(unique_observations):\n",
    "            if i != j:\n",
    "                # Calculate the string similarity for the current pair\n",
    "                seq_sim = string_similarity(obs_i, obs_j)\n",
    "\n",
    "                # Update the combined similarity matrix with the maximum value between existing and sequence matcher\n",
    "                combined_sim_matrix[i, j] = max(combined_sim_matrix[i, j], seq_sim)\n",
    "\n",
    "    return combined_sim_matrix\n",
    "\n",
    "\n",
    "def average_consistency_score(cosine_sim_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the average consistency score based on the cosine similarity matrix and a given threshold.\n",
    "    \"\"\"\n",
    "    num_rows, num_columns = cosine_sim_matrix.shape\n",
    "    inconsistency = 0\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        if np.any(\n",
    "            (cosine_sim_matrix[i] > threshold) & (cosine_sim_matrix[i] <= 1.0000000)\n",
    "        ):\n",
    "            inconsistency += 1\n",
    "\n",
    "    return (num_rows - inconsistency) / num_rows\n",
    "\n",
    "\n",
    "def process_and_calculate_similarity(\n",
    "    dataset_path, column_names, threshold, Stop_Words=[\"the\", \"and\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the dataset, normalize the text, and calculate the similarity scores for multiple columns.\n",
    "    \"\"\"\n",
    "    # Read the dataset from the provided Excel file path\n",
    "    df = read_data(dataset_path)\n",
    "    overall_consistency_scores = []\n",
    "\n",
    "    # Iterate over each specified column\n",
    "    for column_name in column_names:\n",
    "        # Normalize the text in the specified column and store the results in a new column\n",
    "        df[f\"Normalized {column_name}\"] = df[column_name].apply(normalize_text)\n",
    "\n",
    "        # Get unique normalized observations by removing duplicates and NaN values\n",
    "        unique_observations = pd.unique(\n",
    "            df[f\"Normalized {column_name}\"].dropna().values.ravel()\n",
    "        )\n",
    "\n",
    "        # Calculate the cosine similarity matrix for the unique normalized observations\n",
    "        text_sim_matrix = calculate_cosine_similarity(\n",
    "            unique_observations.tolist(), unique_observations.tolist(), Stop_Words\n",
    "        )\n",
    "\n",
    "        # Set the diagonal of the similarity matrix to 0 to ignore self-similarity\n",
    "        np.fill_diagonal(text_sim_matrix, 0)\n",
    "\n",
    "        # Combine text similarity with numeric similarity to get a final similarity matrix\n",
    "        combined_sim_matrix = calculate_combined_similarity(\n",
    "            df, unique_observations, text_sim_matrix\n",
    "        )\n",
    "\n",
    "        # Initialize columns in the dataframe to store the recommended organization matches and all matches\n",
    "        df[f\"Recommended {column_name}\"] = None\n",
    "        df[f\"All Matches {column_name}\"] = None\n",
    "\n",
    "        # Iterate over each normalized organization in the dataframe\n",
    "        for i, norm_org in enumerate(df[f\"Normalized {column_name}\"]):\n",
    "            # Find the index of the current normalized organization in the unique observations\n",
    "            try:\n",
    "                current_index = np.where(unique_observations == norm_org)[0][0]\n",
    "            except IndexError:\n",
    "                df.at[i, f\"Recommended {column_name}\"] = \"No significant match\"\n",
    "                df.at[i, f\"All Matches {column_name}\"] = []\n",
    "                continue\n",
    "\n",
    "            # Get the similarities for the current organization from the combined similarity matrix\n",
    "            similarities = combined_sim_matrix[current_index]\n",
    "\n",
    "            # Find the indices and values of all matching organizations\n",
    "            matched_indices = np.where(similarities >= threshold)[0]\n",
    "            all_matches = [unique_observations[idx] for idx in matched_indices]\n",
    "            all_match_scores = [similarities[idx] for idx in matched_indices]\n",
    "\n",
    "            best_score = 0\n",
    "            best_match = \"No significant match\"\n",
    "\n",
    "            # Extract numbers from the current organization\n",
    "            num_list_current = extract_numbers(norm_org)\n",
    "\n",
    "            for idx in matched_indices:\n",
    "                candidate_match = unique_observations[idx]\n",
    "                num_list_candidate = extract_numbers(candidate_match)\n",
    "\n",
    "                if contains_short_number(num_list_current) or contains_short_number(\n",
    "                    num_list_candidate\n",
    "                ):\n",
    "                    # If short numbers are present, ensure they match; otherwise, skip this match\n",
    "                    if not numbers_match(num_list_current, num_list_candidate):\n",
    "                        continue\n",
    "                    # Recalculate similarity excluding short numbers\n",
    "                    norm_org_no_nums = remove_short_numbers(norm_org)\n",
    "                    candidate_no_nums = remove_short_numbers(candidate_match)\n",
    "                    recalculated_similarity = string_similarity(\n",
    "                        norm_org_no_nums, candidate_no_nums\n",
    "                    )\n",
    "                    if recalculated_similarity > best_score:\n",
    "                        best_score = recalculated_similarity\n",
    "                        best_match = candidate_match\n",
    "                else:\n",
    "                    if similarities[idx] > best_score:\n",
    "                        best_score = similarities[idx]\n",
    "                        best_match = candidate_match\n",
    "\n",
    "            # Assign the best match to the dataframe\n",
    "            if best_score > threshold:\n",
    "                df.at[i, f\"Recommended {column_name}\"] = (\n",
    "                    f\"{best_match} ({best_score:.2f})\"\n",
    "                )\n",
    "            else:\n",
    "                df.at[i, f\"Recommended {column_name}\"] = \"No significant match\"\n",
    "\n",
    "            # Store all matches\n",
    "            df.at[i, f\"All Matches {column_name}\"] = \", \".join(\n",
    "                [\n",
    "                    f\"{match} ({score:.2f})\"\n",
    "                    for match, score in zip(all_matches, all_match_scores)\n",
    "                    if score > threshold\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Calculate the overall consistency score for the current column\n",
    "        consistency_score = average_consistency_score(text_sim_matrix, threshold)\n",
    "        overall_consistency_scores.append(consistency_score)\n",
    "\n",
    "    # Calculate the overall consistency score as the average of individual consistency scores\n",
    "    overall_consistency_score = np.mean(overall_consistency_scores)\n",
    "    df[\"Overall Consistency Score\"] = overall_consistency_score\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Consistency (C1)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=column_names,\n",
    "        threshold=threshold,\n",
    "        score=overall_consistency_score,\n",
    "    )\n",
    "\n",
    "    return overall_consistency_score  # to return the score\n",
    "    # return df #to return the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2 Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compared columns in question must be identical to the ref list, otherwise they will be penalized more harshly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Get names used for a single column\n",
    "def get_names_used_for_column(df, column_name):\n",
    "    unique_observations = pd.unique(df[column_name].dropna().values.ravel())\n",
    "    return unique_observations\n",
    "\n",
    "\n",
    "# Function 2: Calculate Cosine Similarity\n",
    "def calculate_cosine_similarity(text_list, ref_list, Stop_Words):\n",
    "    count_vectorizer = CountVectorizer(stop_words=Stop_Words)\n",
    "    ref_vec = count_vectorizer.fit_transform(ref_list).todense()\n",
    "    ref_vec_array = np.array(ref_vec)\n",
    "    text_vec = count_vectorizer.transform(text_list).todense()\n",
    "    text_vec_array = np.array(text_vec)\n",
    "    cosine_sim = np.round((cosine_similarity(text_vec_array, ref_vec_array)), 2)\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "# Function 3: Average Consistency Score\n",
    "def average_consistency_score(cosine_sim_df, threshold=0.91):\n",
    "    num_rows, num_columns = cosine_sim_df.shape\n",
    "    total_count = 0  # This will count all values above or equal to the threshold\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        if np.max(cosine_sim_df[i]) >= threshold:  # Include all comparisons\n",
    "            total_count += 1\n",
    "    total_observations = num_rows  # Total number of observations\n",
    "    average_consistency_score = total_count / total_observations\n",
    "    return average_consistency_score\n",
    "\n",
    "\n",
    "def process_and_calculate_similarity_ref(\n",
    "    dataset_path,\n",
    "    column_mapping,\n",
    "    ref_dataset_path=None,\n",
    "    threshold=0.91,\n",
    "    Stop_Words=\"activity\",\n",
    "):\n",
    "    # Read the data file\n",
    "    df = read_data(dataset_path)\n",
    "\n",
    "    # Initialize ref_df if a ref dataset is provided\n",
    "    if ref_dataset_path:\n",
    "        df_ref = read_data(ref_dataset_path)\n",
    "        ref_data = True  # Flag to indicate we are using a ref dataset\n",
    "    else:\n",
    "        ref_data = False  # No ref dataset, compare within the same dataset\n",
    "\n",
    "    all_consistency_scores = []\n",
    "\n",
    "    for selected_column, m_selected_column in column_mapping.items():\n",
    "        if ref_data:\n",
    "            # Compare to ref dataset\n",
    "            unique_observations = get_names_used_for_column(df_ref, m_selected_column)\n",
    "        else:\n",
    "            # Use own column for comparison\n",
    "            unique_observations = get_names_used_for_column(df, selected_column)\n",
    "\n",
    "        cosine_sim_matrix = calculate_cosine_similarity(\n",
    "            df[selected_column].dropna(), unique_observations, Stop_Words=Stop_Words\n",
    "        )\n",
    "        column_consistency_score = average_consistency_score(\n",
    "            cosine_sim_matrix, threshold\n",
    "        )\n",
    "        all_consistency_scores.append(column_consistency_score)\n",
    "\n",
    "    # Calculate the average of all consistency scores\n",
    "    overall_avg_consistency = (\n",
    "        sum(all_consistency_scores) / len(all_consistency_scores)\n",
    "        if all_consistency_scores\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # log the results\n",
    "    log_score(\n",
    "        test_name=\"Consistency (C2)\",\n",
    "        dataset_name=get_dataset_name(dataset_path),\n",
    "        selected_columns=column_mapping,\n",
    "        threshold=threshold,\n",
    "        score=overall_avg_consistency,\n",
    "    )\n",
    "\n",
    "    return overall_avg_consistency"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
